{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Metagenome Tutorial \u00b6 These tutorials describe some current approaches for metagenome analysis using a genome-resolved approach. Authors include: Aaron Darling Matt DeMaere The tutorial makes heavy use of a pig metagenomic timeseries and Hi-C dataset that was generated by Daniela Gaio and Kay Anantanawat. Several people have made key contributions to the overarching project in which the pig metagenomic data was generated, including Toni Chapman (NSW DPI), Steven P. Djordjevic (UTS), Michael YZ Liu, Tiziana Zingali, Linda Falconer, Joyce To, and Leigh Monahan.","title":"Welcome to the Metagenome Tutorial"},{"location":"#welcome-to-the-metagenome-tutorial","text":"These tutorials describe some current approaches for metagenome analysis using a genome-resolved approach. Authors include: Aaron Darling Matt DeMaere The tutorial makes heavy use of a pig metagenomic timeseries and Hi-C dataset that was generated by Daniela Gaio and Kay Anantanawat. Several people have made key contributions to the overarching project in which the pig metagenomic data was generated, including Toni Chapman (NSW DPI), Steven P. Djordjevic (UTS), Michael YZ Liu, Tiziana Zingali, Linda Falconer, Joyce To, and Leigh Monahan.","title":"Welcome to the Metagenome Tutorial"},{"location":"anvio/","text":"Visualization of metagenome data \u00b6 In this section we will explore ways to visualize the metagenomic data, with a view toward understanding the MAGs that we have reconstructed from our metagenomes. For this we will use software called anvi'o, a highly versatile visualization and analysis environment for metagenomic (and pan-genomic) data. The anvi'o software is extensively documented on the anvi'o website , and rather than recapitulate all of that material here we will just go through the basic steps involved in getting our data loaded into anvi'o. For further details on using the variety of features in anvi'o you can refer to the above site. Installing anvi'o \u00b6 The simplest way to install anvi'o is via conda: Create a new environment for anvio conda create -y -n anvio5 anvio = 5 .5.0 conda activate anvio5 Notice that this command is slightly different to what we've been using more frequently for conda software installations. In this command we are invoking conda create which creates a new conda environment just for anvi'o. As software tools can sometimes have clashing dependencies, doing this means conda needs only resolve software dependencies for anvio. We're also being explicit about which version of anvio we wish to install ( avio=5.5.0 ). Without a version specifier, conda will install the latest version that it is aware of. Overriding this behaviour can be handy as unexpected changes to such things as the user interface or program output can break scripts. Once the installation has completed, we activate it conda activate anvio5 . Preparing data for anvi'o \u00b6 First we need to prepare our data for use in anvi'o. Because we already have binning results from MetaBAT2 we will ask anvi'o to import those bins rather than computing new bins. This will allow us to use anvi'o to browse the bins and interactively check and refine them as necessary. But before we get to that, we need to get anvi'o to process the metagenome assembly contigs and the bam files of mapped reads: Preparing to use anvi'o anvi-gen-contigs-database -f contigs-fixnames.fa -o contigs.db -n 'A gene school DB' anvi-run-hmms -c contigs.db for bam in ` ls *.bam ` ; do anvi-profile -i $bam -c contigs.db ; done anvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --skip-hierarchical-clustering --skip-concoct-binning The above series of commands will take us from assembly contigs to a working anvi'o database, but there is a lot of compute along the way so if the data set is anything more than extremely trivial you will have to be very patient . The pig metagenome timeseries dataset we are using in this tutorial requires over 900 hours of CPU time to process with the above commands. If you have access to a large multicore machine (or large AWS instance) this process can be sped up by running many threads via the -T command-line parameter. Next, we need to create a file that will allow us to import our MetaBAT2 bins into anvi'o. This is a pretty simple process: Making the anvi'o profile cd contigs-fixnames.fa.metabat-bins/ grep \">\" bin.*fa | perl -p -i -e \"s/bin\\.(.+).fa:>(.+)/\\$2\\tbin_\\$1/g\" > ../binning_results.txt cd .. anvi-import-collection binning_results.txt -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C \"MetaBAT2\" --contigs-mode anvi-summarize -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C MetaBAT2 -o MERGED_SUMMARY The idea is to create a tab-delimited text file with two columns: the first is contig name and the second is the name of the bin that contig belongs to. The command grep \">\" bin.*fa pulls out the contig names from each FastA bin file, and pipes the result to this command perl -p -i -e \"s/(.+).fa:>(.+)/\\$2\\t\\$1/g\" which is using a perl regular expression to extract the contig name (in $2) and bin ID (in $1) and report them in two columns separated by the tab character \\t . The results are saved in a file called binning_results.txt . We can then import the binning results into our anvi'o database with anvi-import-collection , and then compute some useful summaries of the bins with anvi-summarize . Connecting to an anvi'o server \u00b6 When used in interactive mode anvi'o is usually expected to be running locally on your own machine. However, we are working on VMs in the cloud and so we need to start up an anvi'o server on our remote machine and then connect to it with our web browser. Note that this will only work with google chrome browser . anvi'o's interactive mode does not currently work with any other browsers. You can try it of course but you're on your own when something goes wrong (which, in fact, could be said of almost anything in bioinformatics). Starting an interactive anvi'o session anvi-interactive -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 \\ --password-protected -C MetaBAT2 when anvi'o launches it will ask you to provide a password. Make one up, and be sure to choose one you can remember at least long enough to log into the server! Once the server is running you can log into it via the chrome web browser by providing the IP address and port 8080 in the location bar, e.g. http://AA.BB.CC.DD:8080 where AA.BB.CC.DD is the IP of your VM. Alternatively if you are using the a provided workshop VM you can just open (in a new tab) the anvio.html file from Jupyter and it will redirect the browser to port 8080. Refining MAGs with anvi'o \u00b6 While the automated binning process implemented in MetaBAT2 is relatively easy to apply even to large datasets, these methods are not perfect and sometimes they can produce erroneous genome bins. anvi'o offers some useful data visualizations methods that can help us to identify cases where a MAG appears to have dubious features. This might be especially relevant if there is a genome that is particularly relevant for your study, for example a nitrogen fixing organism associated with a plant. You might want to confirm that the genome bin looks correct prior to metabolic analysis, for example, to predict culture conditions for isolating an organism. With anvi'o we can inspect individual genome bins, and even modify them interactively, using the anvi-refine command. As with anvi-interactive above this runs via a web server/client structure, so we can launch it on our VM and connect to it with our browser in the same way. For example if we want to refine bin 42 we would run: Refining bins using anvi'o anvi-refine -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 \\ --password-protected -C MetaBAT2 -b bin_42 and then point our browser at the anvi'o server as noted above. For more details about bin refinement with anvi'o check out the tutorials and notes on the anvi'o website: Refining MAGs with anvi'o Notes on bin refinement with anvi'o Challenge exercises \u00b6 Using anvi-interactive Find a bin with a high predicted redundancy rate and another with a low rate. Then load each bin in anvi-refine . How do their profiles differ? If we had a metagenome with two strains where 80% of the gene content was common to both strains, and the binning software reconstructed the most abundant one as a bin, what would the coverage profile for that bin look like in anvi-refine ? What about the coverage standard deviation? What can Single Nucleotide Variants (SNVs) tell us about a genome bin?","title":"Visualization"},{"location":"anvio/#visualization-of-metagenome-data","text":"In this section we will explore ways to visualize the metagenomic data, with a view toward understanding the MAGs that we have reconstructed from our metagenomes. For this we will use software called anvi'o, a highly versatile visualization and analysis environment for metagenomic (and pan-genomic) data. The anvi'o software is extensively documented on the anvi'o website , and rather than recapitulate all of that material here we will just go through the basic steps involved in getting our data loaded into anvi'o. For further details on using the variety of features in anvi'o you can refer to the above site.","title":"Visualization of metagenome data"},{"location":"anvio/#installing-anvio","text":"The simplest way to install anvi'o is via conda: Create a new environment for anvio conda create -y -n anvio5 anvio = 5 .5.0 conda activate anvio5 Notice that this command is slightly different to what we've been using more frequently for conda software installations. In this command we are invoking conda create which creates a new conda environment just for anvi'o. As software tools can sometimes have clashing dependencies, doing this means conda needs only resolve software dependencies for anvio. We're also being explicit about which version of anvio we wish to install ( avio=5.5.0 ). Without a version specifier, conda will install the latest version that it is aware of. Overriding this behaviour can be handy as unexpected changes to such things as the user interface or program output can break scripts. Once the installation has completed, we activate it conda activate anvio5 .","title":"Installing anvi'o"},{"location":"anvio/#preparing-data-for-anvio","text":"First we need to prepare our data for use in anvi'o. Because we already have binning results from MetaBAT2 we will ask anvi'o to import those bins rather than computing new bins. This will allow us to use anvi'o to browse the bins and interactively check and refine them as necessary. But before we get to that, we need to get anvi'o to process the metagenome assembly contigs and the bam files of mapped reads: Preparing to use anvi'o anvi-gen-contigs-database -f contigs-fixnames.fa -o contigs.db -n 'A gene school DB' anvi-run-hmms -c contigs.db for bam in ` ls *.bam ` ; do anvi-profile -i $bam -c contigs.db ; done anvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --skip-hierarchical-clustering --skip-concoct-binning The above series of commands will take us from assembly contigs to a working anvi'o database, but there is a lot of compute along the way so if the data set is anything more than extremely trivial you will have to be very patient . The pig metagenome timeseries dataset we are using in this tutorial requires over 900 hours of CPU time to process with the above commands. If you have access to a large multicore machine (or large AWS instance) this process can be sped up by running many threads via the -T command-line parameter. Next, we need to create a file that will allow us to import our MetaBAT2 bins into anvi'o. This is a pretty simple process: Making the anvi'o profile cd contigs-fixnames.fa.metabat-bins/ grep \">\" bin.*fa | perl -p -i -e \"s/bin\\.(.+).fa:>(.+)/\\$2\\tbin_\\$1/g\" > ../binning_results.txt cd .. anvi-import-collection binning_results.txt -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C \"MetaBAT2\" --contigs-mode anvi-summarize -p SAMPLES-MERGED/PROFILE.db -c contigs.db -C MetaBAT2 -o MERGED_SUMMARY The idea is to create a tab-delimited text file with two columns: the first is contig name and the second is the name of the bin that contig belongs to. The command grep \">\" bin.*fa pulls out the contig names from each FastA bin file, and pipes the result to this command perl -p -i -e \"s/(.+).fa:>(.+)/\\$2\\t\\$1/g\" which is using a perl regular expression to extract the contig name (in $2) and bin ID (in $1) and report them in two columns separated by the tab character \\t . The results are saved in a file called binning_results.txt . We can then import the binning results into our anvi'o database with anvi-import-collection , and then compute some useful summaries of the bins with anvi-summarize .","title":"Preparing data for anvi'o"},{"location":"anvio/#connecting-to-an-anvio-server","text":"When used in interactive mode anvi'o is usually expected to be running locally on your own machine. However, we are working on VMs in the cloud and so we need to start up an anvi'o server on our remote machine and then connect to it with our web browser. Note that this will only work with google chrome browser . anvi'o's interactive mode does not currently work with any other browsers. You can try it of course but you're on your own when something goes wrong (which, in fact, could be said of almost anything in bioinformatics). Starting an interactive anvi'o session anvi-interactive -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 \\ --password-protected -C MetaBAT2 when anvi'o launches it will ask you to provide a password. Make one up, and be sure to choose one you can remember at least long enough to log into the server! Once the server is running you can log into it via the chrome web browser by providing the IP address and port 8080 in the location bar, e.g. http://AA.BB.CC.DD:8080 where AA.BB.CC.DD is the IP of your VM. Alternatively if you are using the a provided workshop VM you can just open (in a new tab) the anvio.html file from Jupyter and it will redirect the browser to port 8080.","title":"Connecting to an anvi'o server"},{"location":"anvio/#refining-mags-with-anvio","text":"While the automated binning process implemented in MetaBAT2 is relatively easy to apply even to large datasets, these methods are not perfect and sometimes they can produce erroneous genome bins. anvi'o offers some useful data visualizations methods that can help us to identify cases where a MAG appears to have dubious features. This might be especially relevant if there is a genome that is particularly relevant for your study, for example a nitrogen fixing organism associated with a plant. You might want to confirm that the genome bin looks correct prior to metabolic analysis, for example, to predict culture conditions for isolating an organism. With anvi'o we can inspect individual genome bins, and even modify them interactively, using the anvi-refine command. As with anvi-interactive above this runs via a web server/client structure, so we can launch it on our VM and connect to it with our browser in the same way. For example if we want to refine bin 42 we would run: Refining bins using anvi'o anvi-refine -p SAMPLES-MERGED/PROFILE.db -c contigs.db --server-only -P 8080 \\ --password-protected -C MetaBAT2 -b bin_42 and then point our browser at the anvi'o server as noted above. For more details about bin refinement with anvi'o check out the tutorials and notes on the anvi'o website: Refining MAGs with anvi'o Notes on bin refinement with anvi'o","title":"Refining MAGs with anvi'o"},{"location":"anvio/#challenge-exercises","text":"Using anvi-interactive Find a bin with a high predicted redundancy rate and another with a low rate. Then load each bin in anvi-refine . How do their profiles differ? If we had a metagenome with two strains where 80% of the gene content was common to both strains, and the binning software reconstructed the most abundant one as a bin, what would the coverage profile for that bin look like in anvi-refine ? What about the coverage standard deviation? What can Single Nucleotide Variants (SNVs) tell us about a genome bin?","title":"Challenge exercises"},{"location":"assembly/","text":"Metagenome assembly \u00b6 What is assembly? \u00b6 Assembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces. In the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons. When presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it. Generally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology. Instead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells. Assembling metagenomes \u00b6 There are several available metagenome assembly tools. Some examples include metaSPAdes and MEGAHIT . In this tutorial we will use MEGAHIT, and we will use it via a docker container. As such, no installation step is necessary. Coassembly or single sample? \u00b6 Just as important as deciding what assembler to use is deciding on what to assemble . One approach is to assemble all of the metagenome samples together. This can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled. But it can create problems if closely related species or strains exist in the different samples. Limitations of metagenome assembly \u00b6 Current assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample. This is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error. Therefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population. Assembling some example data \u00b6 Get a timeseries dataset cd parallel-fastq-dump/parallel-fastq-dump -t 4 --outdir assembly --split-files --gzip --minSpotId 0 --maxSpotId 50000 \\ -s SRR8960410 -s SRR8960409 -s SRR8960402 -s SRR8960368 -s SRR8960420 \\ -s SRR8960739 -s SRR8960679 -s SRR8960627 -s SRR8960591 -s SRR8960887 The above command will download the first 50000 read-pairs of a set of samples. All of these samples come from the same pig, and were collected at different time points in consecutive weeks. Now we can assemble with megahit: Assemble using megahit singularity exec -B ~/assembly/:/data docker://quay.io/biocontainers/megahit:1.1.3--py36_0 megahit -m 16e9 \\ -1 /data/SRR8960410_1.fastq.gz,/data/SRR8960409_1.fastq.gz,/data/SRR8960402_1.fastq.gz,/data/SRR8960368_1.fastq.gz,/data/SRR8960420_1.fastq.gz,/data/SRR8960739_1.fastq.gz,/data/SRR8960679_1.fastq.gz,/data/SRR8960627_1.fastq.gz,/data/SRR8960591_1.fastq.gz,/data/SRR8960887_1.fastq.gz \\ -2 /data/SRR8960410_2.fastq.gz,/data/SRR8960409_2.fastq.gz,/data/SRR8960402_2.fastq.gz,/data/SRR8960368_2.fastq.gz,/data/SRR8960420_2.fastq.gz,/data/SRR8960739_2.fastq.gz,/data/SRR8960679_2.fastq.gz,/data/SRR8960627_2.fastq.gz,/data/SRR8960591_2.fastq.gz,/data/SRR8960887_2.fastq.gz \\ -o /data/metaasm That's a big command-line, so let's unpack what's happening. First, we're invoking singularity . Singularity is a container service that can download and run programs that have been packaged as containers -- a system that allows all needed dependency software to be specified and obtained automatically. By invoking singularity exec we are saying that we want to run a command inside a container. Simply put it allows us to run the software easily and reliably, avoiding the manual software install process. The container we want to use is specified as docker://quay.io/biocontainers/megahit:1.1.3--py36_0 . This is a docker container, and the docker:// syntax tells singularity that it can download the container from a public server. The :1.1.3--py36_0 specifies the exact version of the megahit container to use. Before the container specification we have the argument -B ~/assembly/:/data . This argument binds the assembly/ directory in our current path to appear as /data inside the running container. Therefore the programs running inside the container (e.g. megahit) will be able to see all the files inside ~/assembly/ at the path /data . Next we have the megahit command line. This includes parameters -1 and -2 with a list of the FastQ files we want to assemble. Finally we ask megahit to save the assembly in the container path /data/metaasm , so it will show up in ~/assembly/metaasm when the container has finished running. At the end of this process we will have a metagenome assembly saved in the file ~/assembly/metaasm/final.contigs.fa . We can use this file for subsequent analyses. Simplifying contig multi-fasta headers \u00b6 One small detail we still need to resolve is the formatting of contig headers within the assembly file. Besides just sequence names (eg >k141_1 ), megahit includes additional information in the faster header (eg >k141_1 flag=1 multi=2.0000 len=315 ). Although this is a valid use of the format, the whitespace can be problematic in downstream analyses, such as visualization with anvi'o. So, as a final step in our assembly process, we'll remove the extra information from the headers with the following commands: Simplifying contig names cd assembly/metaasm # use pattern replacement to remove extra details from fasta headers sed -r 's/(>[^ ]+).*/\\1/' final.contigs.fa > contigs-fixnames.fa","title":"Assembly"},{"location":"assembly/#metagenome-assembly","text":"","title":"Metagenome assembly"},{"location":"assembly/#what-is-assembly","text":"Assembly is the process of reconstructing long DNA sequences from a collection of overlapping shorter pieces. In the context of single genomes (e.g. microbial isolates) the goal is usually to reconstruct a single chromosome and any associated plasmids or other replicons. When presented with metagenomic data though, the microbial community might in reality have millions of genomes present in it. Generally speaking, it is impossible to accurately reconstruct every genome of every cell in a metagenomic sample with current technology. Instead we usually aim to reconstruct chromosome sequences that represent a consensus of many cells.","title":"What is assembly?"},{"location":"assembly/#assembling-metagenomes","text":"There are several available metagenome assembly tools. Some examples include metaSPAdes and MEGAHIT . In this tutorial we will use MEGAHIT, and we will use it via a docker container. As such, no installation step is necessary.","title":"Assembling metagenomes"},{"location":"assembly/#coassembly-or-single-sample","text":"Just as important as deciding what assembler to use is deciding on what to assemble . One approach is to assemble all of the metagenome samples together. This can be useful if there is not much data per sample, or if low abundance organisms exist in the samples that would not have enough coverage in a single sample to be assembled. But it can create problems if closely related species or strains exist in the different samples.","title":"Coassembly or single sample?"},{"location":"assembly/#limitations-of-metagenome-assembly","text":"Current assembly methods are limited in their ability to resolve chromosomes of closely related species and strains in a sample. This is because most assembly methods collapse sequences > 95% or 98% identity into a single contig sequence, usually as a means to cope with sequencing error. Therefore as a consensus representation, these assembled chromosome sequences may mask fine-scale genetic variation in the population.","title":"Limitations of metagenome assembly"},{"location":"assembly/#assembling-some-example-data","text":"Get a timeseries dataset cd parallel-fastq-dump/parallel-fastq-dump -t 4 --outdir assembly --split-files --gzip --minSpotId 0 --maxSpotId 50000 \\ -s SRR8960410 -s SRR8960409 -s SRR8960402 -s SRR8960368 -s SRR8960420 \\ -s SRR8960739 -s SRR8960679 -s SRR8960627 -s SRR8960591 -s SRR8960887 The above command will download the first 50000 read-pairs of a set of samples. All of these samples come from the same pig, and were collected at different time points in consecutive weeks. Now we can assemble with megahit: Assemble using megahit singularity exec -B ~/assembly/:/data docker://quay.io/biocontainers/megahit:1.1.3--py36_0 megahit -m 16e9 \\ -1 /data/SRR8960410_1.fastq.gz,/data/SRR8960409_1.fastq.gz,/data/SRR8960402_1.fastq.gz,/data/SRR8960368_1.fastq.gz,/data/SRR8960420_1.fastq.gz,/data/SRR8960739_1.fastq.gz,/data/SRR8960679_1.fastq.gz,/data/SRR8960627_1.fastq.gz,/data/SRR8960591_1.fastq.gz,/data/SRR8960887_1.fastq.gz \\ -2 /data/SRR8960410_2.fastq.gz,/data/SRR8960409_2.fastq.gz,/data/SRR8960402_2.fastq.gz,/data/SRR8960368_2.fastq.gz,/data/SRR8960420_2.fastq.gz,/data/SRR8960739_2.fastq.gz,/data/SRR8960679_2.fastq.gz,/data/SRR8960627_2.fastq.gz,/data/SRR8960591_2.fastq.gz,/data/SRR8960887_2.fastq.gz \\ -o /data/metaasm That's a big command-line, so let's unpack what's happening. First, we're invoking singularity . Singularity is a container service that can download and run programs that have been packaged as containers -- a system that allows all needed dependency software to be specified and obtained automatically. By invoking singularity exec we are saying that we want to run a command inside a container. Simply put it allows us to run the software easily and reliably, avoiding the manual software install process. The container we want to use is specified as docker://quay.io/biocontainers/megahit:1.1.3--py36_0 . This is a docker container, and the docker:// syntax tells singularity that it can download the container from a public server. The :1.1.3--py36_0 specifies the exact version of the megahit container to use. Before the container specification we have the argument -B ~/assembly/:/data . This argument binds the assembly/ directory in our current path to appear as /data inside the running container. Therefore the programs running inside the container (e.g. megahit) will be able to see all the files inside ~/assembly/ at the path /data . Next we have the megahit command line. This includes parameters -1 and -2 with a list of the FastQ files we want to assemble. Finally we ask megahit to save the assembly in the container path /data/metaasm , so it will show up in ~/assembly/metaasm when the container has finished running. At the end of this process we will have a metagenome assembly saved in the file ~/assembly/metaasm/final.contigs.fa . We can use this file for subsequent analyses.","title":"Assembling some example data"},{"location":"assembly/#simplifying-contig-multi-fasta-headers","text":"One small detail we still need to resolve is the formatting of contig headers within the assembly file. Besides just sequence names (eg >k141_1 ), megahit includes additional information in the faster header (eg >k141_1 flag=1 multi=2.0000 len=315 ). Although this is a valid use of the format, the whitespace can be problematic in downstream analyses, such as visualization with anvi'o. So, as a final step in our assembly process, we'll remove the extra information from the headers with the following commands: Simplifying contig names cd assembly/metaasm # use pattern replacement to remove extra details from fasta headers sed -r 's/(>[^ ]+).*/\\1/' final.contigs.fa > contigs-fixnames.fa","title":"Simplifying contig multi-fasta headers"},{"location":"binning/","text":"MAGs from binning metagenomes \u00b6 What is a MAG? \u00b6 A MAG (metagenome-assembled genome) is a putative genome, reconstructed from metagenomic data. Because these genomes do not derive from a clonal culture they typically represent a consensus of the genomes of many closely related cells in one or more metagenomic samples, and for this reason they are sometimes called population genomes . It is important to remember that, because of the way MAGs are inferred from the data, the resulting sequence is usually fragmentary and may not accurately represent the genome of any cell in the community. It is merely an average estimate, and there are well-known problems with taking averages, see for example the wikipedia page on Simpson's paradox to get an idea of how averages can go wrong. What is a good MAG? \u00b6 The international genomics community has made an effort to define quality standards for MAGs via the Minimum Information about a Metagenome Assembled Genome (MIMAG) standards. The basic idea is to require that a MAG possess a certain minimum level of estimated completeness and maximal level of estimated contamination for it to be considered as one of \"Low-quality\", \"Medium-quality\", \"High-quality\", or \"Finished\". See Table 1 in the above-linked paper for the full details. Generally speaking it is hard to achieve \"High-quality\" and nearly impossible to get \"Finished\" with short read sequencing data alone. Long read metagenome data has been shown to produce results in these categories, although it can be difficult to obtain sufficient DNA for those methods, and they currently remain more expensive than short read sequencing. Making MAGs \u00b6 The process of reconstructing genomes from a metagenome is often referred to as metagenome binning or just binning , from the process of assigning contigs to one 'bin' per genome. There are many binning tools available to extract MAGs from metagenome assemblies. When timeseries data is available, MetaBAT2 is a good choice because it is both easy to use and offers good performance. MetaBAT2 can be installed via conda as follows: Install MetaBAT2 conda install -y -n workshop metabat2 MetaBAT2 input data \u00b6 As input, MetaBAT2 requires reads from each of the shotgun metagenome samples to be mapped back to the metagenome assembly. We will not cover the read mapping process in this tutorial, but it can be carried out using standard mapping software such as bwa mem or bowtie2 . Be sure to sort and index the bam files with samtools prior to running metabat. Assuming we have bam files of mapped reads and the metagenome assembly available in a directory called /data/assembly we can compute genome bins as follows: Analyse the assembled timeseries data-set with runMetaBat.sh # create a folder for metabat output mkdir ~/metabat ; cd ~/metabat # analyze the set of bams against the metagenome runMetaBat.sh /data/assembly/contigs-fixnames.fa /data/assembly/*.bam Depending on how much data you've got this can take a long time to compute. Luckily MetaBAT2 has a good parallel implementation so it can go faster if you run on a large multi-core machine. At the end of the process MetaBAT2 will produce a directory called contigs-fixnames.fa.metabat-bins , which contains one FastA file of contigs for each genome bin that it inferred.","title":"Binning"},{"location":"binning/#mags-from-binning-metagenomes","text":"","title":"MAGs from binning metagenomes"},{"location":"binning/#what-is-a-mag","text":"A MAG (metagenome-assembled genome) is a putative genome, reconstructed from metagenomic data. Because these genomes do not derive from a clonal culture they typically represent a consensus of the genomes of many closely related cells in one or more metagenomic samples, and for this reason they are sometimes called population genomes . It is important to remember that, because of the way MAGs are inferred from the data, the resulting sequence is usually fragmentary and may not accurately represent the genome of any cell in the community. It is merely an average estimate, and there are well-known problems with taking averages, see for example the wikipedia page on Simpson's paradox to get an idea of how averages can go wrong.","title":"What is a MAG?"},{"location":"binning/#what-is-a-good-mag","text":"The international genomics community has made an effort to define quality standards for MAGs via the Minimum Information about a Metagenome Assembled Genome (MIMAG) standards. The basic idea is to require that a MAG possess a certain minimum level of estimated completeness and maximal level of estimated contamination for it to be considered as one of \"Low-quality\", \"Medium-quality\", \"High-quality\", or \"Finished\". See Table 1 in the above-linked paper for the full details. Generally speaking it is hard to achieve \"High-quality\" and nearly impossible to get \"Finished\" with short read sequencing data alone. Long read metagenome data has been shown to produce results in these categories, although it can be difficult to obtain sufficient DNA for those methods, and they currently remain more expensive than short read sequencing.","title":"What is a good MAG?"},{"location":"binning/#making-mags","text":"The process of reconstructing genomes from a metagenome is often referred to as metagenome binning or just binning , from the process of assigning contigs to one 'bin' per genome. There are many binning tools available to extract MAGs from metagenome assemblies. When timeseries data is available, MetaBAT2 is a good choice because it is both easy to use and offers good performance. MetaBAT2 can be installed via conda as follows: Install MetaBAT2 conda install -y -n workshop metabat2","title":"Making MAGs"},{"location":"binning/#metabat2-input-data","text":"As input, MetaBAT2 requires reads from each of the shotgun metagenome samples to be mapped back to the metagenome assembly. We will not cover the read mapping process in this tutorial, but it can be carried out using standard mapping software such as bwa mem or bowtie2 . Be sure to sort and index the bam files with samtools prior to running metabat. Assuming we have bam files of mapped reads and the metagenome assembly available in a directory called /data/assembly we can compute genome bins as follows: Analyse the assembled timeseries data-set with runMetaBat.sh # create a folder for metabat output mkdir ~/metabat ; cd ~/metabat # analyze the set of bams against the metagenome runMetaBat.sh /data/assembly/contigs-fixnames.fa /data/assembly/*.bam Depending on how much data you've got this can take a long time to compute. Luckily MetaBAT2 has a good parallel implementation so it can go faster if you run on a large multi-core machine. At the end of the process MetaBAT2 will produce a directory called contigs-fixnames.fa.metabat-bins , which contains one FastA file of contigs for each genome bin that it inferred.","title":"MetaBAT2 input data"},{"location":"everythingelse/","text":"What we didn't cover \u00b6 Experimental design \u00b6 There are an almost endless array of questions that can be addressed with metagenomics and in this tutorial material we have only begun to scratch the surface of what is possible. As a research tool, metagenomics is now beginning to move away from being purely discovery-oriented towards becoming a tool that is used to test specific hypotheses. There are many, many things to consider when designing an experiment that involves metagenomics as a means to test a hypothesis. First among these is whether the effect would be measurable via metagenomic data, e.g. is metagenomics the right tool for the job? Then comes questions like: How deeply will you sequence? How many samples will you need? Will samples be structured as a time-series, or transect of some kind? How much background variation exists in the metagenomes that will be sampled? And based on the above, using a particular set of metagenome analysis tools, what is our statistical power to reject the hypothesis in question? Simulation for experimental design \u00b6 One way to design and power a large metagenomic experiment is to carry out a computational simulation of the metagenomic sequencing that follows the experimental structure. While this may seem tedious, I would argue that if the experiment is worth doing, then it is almost certainly worth doing a simulation study first. There are a number of advantages to this kind of an approach. For example: Hidden obstacles to the data analysis will be identified a priori , before the long and expensive experimental work and data generation begins. A data analysis workflow can be developed up front, so that when the data arrives the work to analyse it becomes simpler. The experiment can be rationally designed, with clear expectations about the number and type of samples required to measure an effect of a particular size. One of the challenges to taking this approach is that reasonable simulation parameters may not be known. If you're working in a well-studied microbial ecosystem like the human gut it may be possible to design an experiment entirely using knowledge gained from existing public datasets. But if you're working in a more obscure system then there may not be much, if any, public data to use for experimental design. In general the solution to this problem requires an iterative process, in which an initial small batch of pilot data is created, from which reasonable parameters can be estimated for the design of a larger study. Resolving strain mixtures \u00b6 When a metagenomic sample contains genomes from more than one strain of the same species, or from two closely related species, it can cause the assembly to become highly fragmentary. The resulting assembly contigs can be very difficult to analyse with standard genome binning methods, for multiple reasons. First, many binning tools will not process contigs if they are below a particular size. Depending on the exact level of sequence identity among the two strains, only a small fraction of the genome might be present in contigs above the lower size limit. Second, some of the contigs can represent a coassembly of the two strains. Not only can this result in contig sequences that look unlike any of the individual genomes, but it also poses a problem for standard metagenome binning software because the contig belongs in more than one bin, yet these softwares can only assign contigs to single bins. While there has been some work to try to resolve strain mixtures from metagenome assemblies, such as that implemented in the DESMAN software , the problem remains challenging and better solutions are needed. Alpha and beta diversity, ecological networks \u00b6 Although people usually carry out 16S amplicon sequencing to answer questions about microbial ecology it is also possible to address these with metagenomic data. There are a range of computational methods and tools for taxonomy-driven metagenome community profiling. Some of these were reviewed and evaluated in the CAMI publication . The taxonomy-based methods are limited, however, in that they can only resolve named taxonomic groups. Much of the microbial diversity on our planet remains undescribed, and this is where phylogenetic methods have the potential to greatly outperform the taxonomic methods. Phylogenetic methods don't depend on human-curated taxonomic structures, and therefore are capable of analysing wholly novel groups of organisms. The equivalent class of methods in the 16S amplicon sequencing world are the reference-free de novo OTU or Amplicon Sequence Variant (ASV) analysis methods. The list goes on \u00b6 There are many, many more ways to analyze metagenomic data. Once MAGs have been reconstructed from the metagenomes pretty much any of the analyses that could be carried out on isolate genomes can be applied. There are also a range of metagenome-specific analyses that could be applied to MAGs, such as association studies, phylogenetic studies, functional and metabolic analyses, and more.","title":"Everything Else"},{"location":"everythingelse/#what-we-didnt-cover","text":"","title":"What we didn't cover"},{"location":"everythingelse/#experimental-design","text":"There are an almost endless array of questions that can be addressed with metagenomics and in this tutorial material we have only begun to scratch the surface of what is possible. As a research tool, metagenomics is now beginning to move away from being purely discovery-oriented towards becoming a tool that is used to test specific hypotheses. There are many, many things to consider when designing an experiment that involves metagenomics as a means to test a hypothesis. First among these is whether the effect would be measurable via metagenomic data, e.g. is metagenomics the right tool for the job? Then comes questions like: How deeply will you sequence? How many samples will you need? Will samples be structured as a time-series, or transect of some kind? How much background variation exists in the metagenomes that will be sampled? And based on the above, using a particular set of metagenome analysis tools, what is our statistical power to reject the hypothesis in question?","title":"Experimental design"},{"location":"everythingelse/#simulation-for-experimental-design","text":"One way to design and power a large metagenomic experiment is to carry out a computational simulation of the metagenomic sequencing that follows the experimental structure. While this may seem tedious, I would argue that if the experiment is worth doing, then it is almost certainly worth doing a simulation study first. There are a number of advantages to this kind of an approach. For example: Hidden obstacles to the data analysis will be identified a priori , before the long and expensive experimental work and data generation begins. A data analysis workflow can be developed up front, so that when the data arrives the work to analyse it becomes simpler. The experiment can be rationally designed, with clear expectations about the number and type of samples required to measure an effect of a particular size. One of the challenges to taking this approach is that reasonable simulation parameters may not be known. If you're working in a well-studied microbial ecosystem like the human gut it may be possible to design an experiment entirely using knowledge gained from existing public datasets. But if you're working in a more obscure system then there may not be much, if any, public data to use for experimental design. In general the solution to this problem requires an iterative process, in which an initial small batch of pilot data is created, from which reasonable parameters can be estimated for the design of a larger study.","title":"Simulation for experimental design"},{"location":"everythingelse/#resolving-strain-mixtures","text":"When a metagenomic sample contains genomes from more than one strain of the same species, or from two closely related species, it can cause the assembly to become highly fragmentary. The resulting assembly contigs can be very difficult to analyse with standard genome binning methods, for multiple reasons. First, many binning tools will not process contigs if they are below a particular size. Depending on the exact level of sequence identity among the two strains, only a small fraction of the genome might be present in contigs above the lower size limit. Second, some of the contigs can represent a coassembly of the two strains. Not only can this result in contig sequences that look unlike any of the individual genomes, but it also poses a problem for standard metagenome binning software because the contig belongs in more than one bin, yet these softwares can only assign contigs to single bins. While there has been some work to try to resolve strain mixtures from metagenome assemblies, such as that implemented in the DESMAN software , the problem remains challenging and better solutions are needed.","title":"Resolving strain mixtures"},{"location":"everythingelse/#alpha-and-beta-diversity-ecological-networks","text":"Although people usually carry out 16S amplicon sequencing to answer questions about microbial ecology it is also possible to address these with metagenomic data. There are a range of computational methods and tools for taxonomy-driven metagenome community profiling. Some of these were reviewed and evaluated in the CAMI publication . The taxonomy-based methods are limited, however, in that they can only resolve named taxonomic groups. Much of the microbial diversity on our planet remains undescribed, and this is where phylogenetic methods have the potential to greatly outperform the taxonomic methods. Phylogenetic methods don't depend on human-curated taxonomic structures, and therefore are capable of analysing wholly novel groups of organisms. The equivalent class of methods in the 16S amplicon sequencing world are the reference-free de novo OTU or Amplicon Sequence Variant (ASV) analysis methods.","title":"Alpha and beta diversity, ecological networks"},{"location":"everythingelse/#the-list-goes-on","text":"There are many, many more ways to analyze metagenomic data. Once MAGs have been reconstructed from the metagenomes pretty much any of the analyses that could be carried out on isolate genomes can be applied. There are also a range of metagenome-specific analyses that could be applied to MAGs, such as association studies, phylogenetic studies, functional and metabolic analyses, and more.","title":"The list goes on"},{"location":"hic/","text":"Exploiting DNA-DNA Proximity \u00b6 What is Metagenomic Hi-C? \u00b6 As the name suggests, Metagenomic Hi-C is an adaptation of the original Hi-C protocol -- used for the 3-dimensional study of chromosome structure [1] -- to shotgun metagenome analysis. Metagenomic Hi-C is a promising addition to our analysis toolbox, as it makes it possible for genome binning to be carried out accurately and precisely from a single sample rather than requiring large collection of samples (such as in timeseries binning). The Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity in vivo . With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently observed interactions are usually those between nearby loci on the same chromosome (intra-chromosomal). Though the rate of interaction between loci falls off quickly with separation, it does not go to zero and long-distance contacts within a chromosome remain observable. Less frequent than intra-chromosomal interactions are those between separate molecules (chromosomes, plasmids, etc) within the same cell (inter-chromosomal). To what degree we see inter-chromosomal interactions depends greatly on the organism(s). The least common are inter-cellular interactions and are often seen at rates far below those found within a cell. The proximity information captured in Hi-C sequencing data can be readily applied to improve draft genome and metagenome assemblies. In metagenomics we use the proximity information to infer which assembly fragments (contigs) originated from the same cell (or chromosome). Though the power of the \"proximity signal\" should ultimately make it possible to infer individual genotypes, current tools stop at the same resolution as timeseries binning, associating assembly fragments into MAGs. The Hi-C Library Protocol \u00b6 Major steps of the Hi-C protocol Basic Concept \u00b6 Though Hi-C sequencing data is generated using conventional high-throughput Illumina paired-end sequencing, the more complicated Hi-C protocol produces a very different library. Unlike traditional shotgun sequencing, where read pairs originate from nearby regions on the same contiguous DNA molecule (i.e. chromosome, plasmid, etc), the Hi-C protocol can generate read-pairs from any two strands of DNA that were in close physical proximity within the cell. So long as the two DNA loci interact, regardless of their location with the genome, there is a chance of producing a Hi-C read-pair from that interaction. Hi-C read pairs can therefore originate from far-away regions of the same molecule or from entirely different molecules. To achieve this, the Hi-C library protocol involves several steps upstream of Illumina adapter ligation and is more challenging to execute in the lab. As a result, commercial kits have appeared which aim simplify the process and improve quality and consistency of the resulting libraries. Some commercial companies offering varieties of Hi-C kits and services \u00b6 Phase Genomics Arima Genomics Dovetail Genomics Rough Protocol Outline \u00b6 DNA Fixation: Beginning with intact cells, the first step of the Hi-C protocol is formalin fixation. The act of cross-linking \"locks in\" close-by conformation arrangements within the DNA that existed in the cells at the time of fixation. Cell Lysis: The cells are lysed and DNA-protein complexes extracted and purified. Restriction Digest: The DNA is digested using a restriction endonuclease. In metagenomic Hi-C, enzymes with large overhangs and 4-nt recognition sites are common choices (i.e. Sau3AI, MluCI, DpnII). Differences in the GC bias of the recognition site and the target DNA is an important factor, as inefficient digestion will produce fewer Hi-C read-pairs. Biotin Tagging: The overhangs produced during digestion are end-filled with biotinylated nucleotides. Free-end Ligation: in dilute conditions or immobilised on a substrate, free-ends protruding from the DNA-protein complexes are ligated. This stochastic process favours any two ends which were nearby within the complex, however random ligation and self-ligation can occur and the reads resulting from such events create noise/error in the data. Crosslink Reversal: The bonds created by formalin fixation are removed, allowing the now free DNA to be purified. Proteinase digestion is common following this step. Un-ligated End Clean-up: Free-ends which failed to form ligation products are unwanted in subsequent steps but could still be biotin tagged. To minimise their inclusion, a light exonuclease 3' to 5' favoured chew-back can be applied. DNA Shearing: With ligation completed, the DNA is mechanically sheared and the size range of the resulting fragment selected suitability with Illumina paired-end sequencing. DNA repair: Sonication can lead to damaged ends and nicks, which can be repaired. Proximity Ligation Enrichment: Biotin tagged fragments are pulled down using affinity purification. Adapter Ligation: Illumina paired-end adapter ligation and associated steps to produce a sequencing library are now applied. Quality Control: is my library ok? \u00b6 Due to the highly variable nature of samples taken directly from an environment, the possible presence of enzyme inhibitors, and other reasons, the metagenomic Hi-C protocol may not always work with high efficiency, or at all. Problems with sample processing can sometimes be identified prior to sequencing, for example if the measured library yield is very low. But other problems may be difficult to diagnose prior to sequencing, for example, it is very difficult to estimate the efficiency of proximity ligation and thus the fraction of sequencing reads that will be Hi-C reads without actually sequencing the sample. The reads that are not Hi-C reads are essentially conventional shotgun read-pairs, which in the context of Hi-C are uninformative. The value of the proximity information contained within a Hi-C read-set is high enough that even with a low efficiency library, it can be worthwhile to compensate for low efficiency by sequencing more deeply to obtain more Hi-C read pairs. Thus, determining the percentage of Hi-C read-pairs in the library is important, because it can tell us how deeply we need to sequence our library. Rather than submit a library to a large and costly sequencing run immediately, a small pilot sequencing run can be sufficient to confidently estimate the Hi-C efficiency. Armed with this information, a researcher can make informed decisions about further action; such as whether a candidate Hi-C library should be fully sequenced and to what depth. We now discuss various ways to estimate the fraction of Hi-C reads in a library. Evidence of Proximity Ligation \u00b6 Long-range pairs \u00b6 The separation distance for intra-chromosomal Hi-C read-pairs is bounded only by the length of the chromosome. This is unlike shotgun read-pairs whose separation is rarely more than a thousand nucleotides due to chemistry limitations on Illumina instruments. Though it is less direct than looking for evidence of cut-site duplication events, a simple count of the number of read pairs than map far apart can be used to infer the percentage of Hi-C read-pairs. The main limitation of this approach is that it requires a high quality reference assembly against which the Hi-C read pairs can be mapped. In many metagenomic projects, such an assembly is not actually available, either because a good quality shotgun metagenome dataset has not yet been generated for the sample or because there are no close reference genomes in public databases for the organisms in the sample. Counting the number of of pairs which map at long-range can as be used to infer the percentage of Hi-C read-pairs. Proximity ligation junctions \u00b6 In the Hi-C protocol outlined above, the steps of creating free-ends through enzymatic digestion, subsequent end-repair, and finally re-ligation introduces an artifact at the junction point. This artifact is a short sequence duplication and the exact sequence enzyme dependent. The duplication is produced when the cleavage site overhangs are subjected to end-repair. When the repaired blunt ends are subsequently ligated, their junction contains two copies of the overhang sequence. Proximity junction for the enzyme Sau3AI Sau3AI is a 4-cutter with a 4nt overhang. Native DNA 5`-XXXXGATCYYYY-3' 3'-xxxxCTAGyyyy-5' Free-ends post cleavage have overhangs 5`-XXXX GATCYYYY-3' 3'-xxxxCTAG yyyy-5' Blunt ends after end-repair with biotinylated nucleotides 5`-XXXXGATC GATCYYYY-3' 3'-xxxxCTAG CTAGyyyy-5' Free-end ligation produces a duplication at the junction 5`-XXXXGATCGATCYYYY-3' 3'-xxxxCTAGCTAGyyyy-5' After the steps of DNA shearing and size selection, fragments which eventually go on to DNA sequencing can contain a junction at any point along their extent. An Illumina paired-end sequencing run, which generates reads at either end of a fragment, thus has two chances to read through the junction. This approach has two main drawbacks: If the fragments in the sequencing library are long relative to the read length, there may be junctions within the fragment that remain unobserved, and this fact must be corrected for in the estimate. If the sample was degraded there may have been free ends that were not created by an enzyme cut. These could become proximity-ligated, and therefore ligation junctions may exist that do not contain the obvious junction sequence. Searching the read-set for examples of this junction sequence is one means of measuring the percentage of Hi-C read-pairs in a given library. Using a QC Tool \u00b6 Though the methodology of Hi-C QC has yet to achieve standardisation, a number of tools exist which offer forms of QC testing. These tools can be found as components of Hi-C analysis pipelines ( HiCpipe ), embedded in tool-suites ( HiCExplorer ) and as stand-alone command-line tools ( hic_qc , qc3C ) [2-5]. In this tutorial we will use qc3C to assess Hi-C library quality in two different ways: Read mapping based analysis k -mer based analysis. Get some Hi-C data and tools \u00b6 For a single timepoint, we'll first download a small Hi-C read-set and its associated shotgun read-set. Also, we'll pull down the Docker images for qc3C (quality testing) and bin3C (metagenome binning) Get some Hi-C data and the qc3C and bin3C docker images # download the read-set parallel-fastq-dump/parallel-fastq-dump -s SRR9323809 -s SRR8960211 --threads 4 --outdir hic_data/ \\ --minSpotId 0 --maxSpotId 1000000 --split-files --gzip # fetch the qc3C image docker pull cerebis/qc3c:alpine # fetch the bin3c image docker pull cerebis/bin3c:latest Create a metagenome assembly and map Hi-C reads \u00b6 The shotgun dataset has been limited to 1M read-pairs, this is only 1/165 th of the total. Obviously this reduction in total coverage will lead to a much sparser sampling of the community and a more fragmented assembly. Nevertheless, it is enough data to demonstrate the concept of QC and Hi-C metagenome binning. Lets assemble the shotgun data and map both library types to the resulting contigs. Create a SPAdes assembly # enter the hic data directory cd hic_data # launch spades in metagenomic mode docker run -v $PWD :/opt/app-root cerebis/bin3c:latest spades.py --meta -1 SRR8960211_1.fastq.gz -2 SRR8960211_2.fastq.gz -o asm Once we have our assembly, we can map the Hi-C reads to the contigs. This mapping forms the basis of what binning tools like bin3C use to infer proximity associations between DNA loci. Map Hi-C reads to assembly contigs # index the contig fasta docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bwa index asm/contigs.fasta # map hi-c reads to the contigs docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR9323809_1.fastq.gz SRR9323809_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o hic_to_ctg.bam -\" # map shotgun reads to the contigs docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR8960211_1.fastq.gz SRR8960211_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o wgs_to_ctg.bam -\" # return to your home directory cd Breaking down the read mapping command We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/bin3c:latest ). Finally we include the actual call, which involves pipes between bwa and samtools . To achieve this we pass the bash shell our complex command, which it then executes within the container. BAM mode analysis with qc3C \u00b6 We begin with the conceptually simpler approach to Hi-C QC, counting the number of long-range read-pairs from a Hi-C read-set. As a rule of thumb, to work well with Illumina sequencing, the physical size of fragments is likely to be <1000nt. We shall therefore simply count the number of pairs which map with a separation >1000, >5000 and >10,000 nt. As the mapped read-pair separation grows beyond 1000nt we expect an increasingly large proportion of these pairs will be due to Hi-C proximity ligation. Conversely, when analysing a shotgun read-set, we expect to see very few. Run a BAM based analysis # enter the hic data folder cd hic_data # perform bam based QC analysis on Hi-C data docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b hic_to_ctg.bam\" # now perform the analysis on shotgun data docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b wgs_to_ctg.bam\" # return to your home directory cd Estimation of fragment size \u00b6 As we are not sure of the mean insert length, we guess it to be 400nt. This is one handy benefit of the BAM mode approach, we can infer a reasonably good estimate of the fragment (insert) length. In k -mer mode, having an accurate value becomes important if we wish to estimate the probable fraction of unobserved proximity junctions. That is to say, for long fragments and short reads, much of each fragment goes unsequenced and so we will miss the evidence of junctions. In our resulting log from above, qc3C reports an observed mean fragment length of 445nt for the Hi-C read-set. INFO | 2019-06-23 18:55:18,652 | qc3C.bam_based | Observed mean of short-range pair separation: 445nt Looking at the results \u00b6 The last three lines of the output from qc3C report the absolute number and relative fraction of read-pairs that mapped far apart. For the Hi-C library, the fraction of pairs separated by more than 10kb was ~2.6%, while in contrast for the known shotgun library the fraction was <0.006%. Hi-C result INFO | 2019-11-19 08:31:13,975 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-11-19 08:31:13,975 | qc3C.bam_based | Number of observed pairs: 366, 169, 84 INFO | 2019-11-19 08:31:13,975 | qc3C.bam_based | Relative fraction of all cis: 0.1149, 0.05308, 0.02638 INFO | 2019-11-19 08:31:13,976 | qc3C.bam_based | Relative fraction of all pairs: 0.04934, 0.02278, 0.01132 Shotgun result INFO | 2019-11-19 08:31:42,171 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-11-19 08:31:42,171 | qc3C.bam_based | Number of observed pairs: 284, 43, 23 INFO | 2019-11-19 08:31:42,172 | qc3C.bam_based | Relative fraction of all cis: 0.0007268, 0.0001101, 5.886e-05 INFO | 2019-11-19 08:31:42,172 | qc3C.bam_based | Relative fraction of all pairs: 0.0003572, 5.408e-05, 2.893e-05 It is important to keep in mind that as the assembly was made from a shotgun dataset downsampled to very low coverage, the length of contigs is significantly lower than would have been obtained with the full dataset. This will impair the odds of seeing pairs which map far away. Despite this, the 98% of pairs in the shotgun library map to the same contig sequence. In contrast, 73% of Hi-C pairs mapped to the same contig. Overall, for BAM mode, the reliability of results is connected to the quality and completeness of the available references. Breaking down the invocation of qc3C We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to qc3C , along with its own options. Potential Concerns Requires a reference sequence: a completed genome a draft assembly. May not be an option when: no reference is available only a very highly fragmented draft exists Computational requirements: create the shotgun assembly map Hi-C reads to the reference The distribution of assembly contig lengths will affect the maximum mapping distance. k -mer mode analysis with qc3C \u00b6 To analyze our small Hi-C read-set using a k -mer approach, we will first create a k -mer library using Jellyfish. Since this Hi-C experiment used the 4-cutter Sau3AI, which produces an 8-mer junction, we'll use a mer size of 24 for the library. This will give us 8 nucleotides either side of any prospective junction, for specificity. Choosing how large a k -mer size to use for the library is a trade-off between computational complexity and minimising false discovery. You can try sizes bigger and smaller. Running k-mer based analyses On the Hi-C data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files rm -f gen_fq && for fn in ` ls SRR9323809*fastq.gz ` ; do echo gzip -dc $fn >> gen_fq ; done # use jellyfish to create a 24-mer library from FastQ reads docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_fq -o hic24.jf # perform k-mer based QC analysis docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 445 -s 1234 -e Sau3AI -l hic24.jf -r SRR9323809_1.fastq.gz -r SRR9323809_2.fastq.gz\" # return back to your home directory cd Now on the shotgun data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files echo \"gzip -dc SRR8960211_1.fastq.gz\" > gen_sg # use jellyfish to create a 24-mer library from FastQ reads docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_sg -o wgs24.jf # perform k-mer based QC analysis docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 306 -s 1234 -e Sau3AI -l wgs24.jf -r SRR8960211_1.fastq.gz -p 0.05\" # return back to your home directory cd Looking at the results \u00b6 Looking at the inferred fraction of true Hi-C read-pairs, we can immediately see that the low coverage shotgun assembly appears to have drastically skewed the estimate of signal (2.7% vs 5.8%). When estimating Hi-C content using an assembly based method, we require a decent assembly. After adjustment for the unobserved fraction (0.32), the k-mer based estimate of signal is increased to 7.6%. In contrast, for the conventional shotgun data-set we should expect an estimate near zero and we see a fairly low value of 0.094%. This tells us that were we to see such low values for a Hi-C library, something has gone seriously wrong with the library prep and further sequencing would be best abandoned. As another check, if we look further up the log we can see that the number of observed junctions was 0.098%, while we could expect 0.0015% purely by random chance. Although this difference might appear significant, it is put clear in perspective when compared to the Hi-C result, where 5.9% of reads contained the junction sequence. This Hi-C library appears to have a reasonable fraction of Hi-C reads. If it were a pilot sequencing run, this result would be sufficient evidence to commit to a larger sequencing run. Hi-C result INFO | 2019-11-19 08:35:53,320 | qc3C.kmer_based | Number of unique observations used in empirical p-value: wgs 348,653, hic 21,592 INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 5.753 \u00b1 0.004587 % INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | Observed mean read length for paired reads: 151nt INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | For supplied insert length of 445nt, estimated unobserved fraction: 0.3213 INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 7.601 \u00b1 0.006062 % Shotgun result INFO | 2019-11-19 08:37:46,411 | qc3C.kmer_based | Number of unique observations used in empirical p-value: wgs 49,715, hic 49 INFO | 2019-11-19 08:37:46,433 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 0.09371 \u00b1 0.003008 % INFO | 2019-11-19 08:37:46,434 | qc3C.kmer_based | Observed mean read length for paired reads: 151nt INFO | 2019-11-19 08:37:46,434 | qc3C.kmer_based | For supplied insert length of 306nt, estimated unobserved fraction: 0.01307 INFO | 2019-11-19 08:37:46,434 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 0.09493 \u00b1 0.003048 % Breaking down the invocation of jellyfish We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to jellyfish , along with its own options. K-mer mode requirements No-assembly or reference required Requires making a k-mer library from FastQ reads Analyse FastQ reads against library Metagenome Binning with Hi-C Data \u00b6 As we saw by processing timeseries data with MetaBAT2 , metagenome binning can also be performed using Hi-C. The primary difference here is that accurate and precise binning is possible from a single timepoint, so long as you have you have both a shotgun and Hi-C read-set. Currenty, only a few tools exist which were made specifically for this purpose ( bin3C , ProxiMeta ) [6,7]. In this tutorial we will use bin3C, as it is open-source and easy to use. With a set of reference sequences and Hi-C to reference BAM file in hand (from above), a bin3C analysis has two stages: first create the contact map, second cluster the contact map. We also need to know what enzyme(s) was used in generating the library. It is becoming more common to generate libraries from two enzymes with differing GC biases to improve Hi-C coverage within a community. In such cases, we simply specify them when creating the map, as here. Perform Hi-C metagenome binning # enter hic data directory cd hic_data # Make contact map docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C mkmap -v --clobber -e Sau3AI -e MluCI --eta --bin-size 5000 asm/contigs.fasta hic_to_ctg.bam bin3C_out # Cluster contact map docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C cluster -v --clobber --only-large bin3C_out/contact_map.p.gz bin3C_out # return to your home directory cd Looking at the results \u00b6 Our example data-set was insufficient Unfortunately, the example data we have used here is insufficient to get a useful result from bin3C. Neither the shotgun, nor the Hi-C data-set was sequenced deeply enough. In reality, we artificially limited the depth when downloading the FastQ data with --max-spot . Were we to go back and obtain the full data-sets, we would achieve a much better result (see the figure below). After bin3C completes the clustering step, the three outputs which are possibly the most interesting are: The comma delimited cluster_report.csv table the cluster_report details characteristics of each identified cluster. An id, name, number of contigs, total extent, mean GC and coverage, etc. Mean coverage is a reasonable gauge of relative abundance A qualitative overview of the result in the form of a heatmap cluster_plot.png contigs are organized by cluster, followed by descending length a pixel represents a 5kb bin and therefore contigs are represented proportional to their length. Clustered sequence data the fasta/ directory contains cluster FastA data The cluster plot is a visual representation of the clustered contact map. As a metagenomic assembly can easily contain 50k to 100k contigs, which as a NxN image would be very large, the plot's resolution is constrained by default to 4000x4000 pixels to conserve memory. Once organised, a good Hi-C read-set will have produced a map that is clearly in block-diagonal form with high contrast. Also, a significant proportion of the map's field is should be dark, reflecting the fact that inter-cellular interactions are rare. In cases where there was insufficient signal, there will only be a few (if any) large clusters and they will look sparse internally. Example cluster plot of a human microbome Right click to get a larger image MAG clusters of a single-timepoint human faecal microbiome (PRJNA413092) as determined by bin3C . The shotgun read-set consisted of 249M pairs, while the Hi-C library contained 41M pairs (~10% Hi-C). Of the identified MAGs, 55 where >90% complete, <5% contamination as estimated by CheckM [6]. If you are familiar with the appearance of a single-genome microbial contact map, at sufficient zoom you may notice that the individual clusters do not look the same. This is due to the fact that the proper ordering of contigs (scaffolding) within the clusters has not been established. After reaching this point, standard downstream analyses would be to profile the taxonomic composition of the clusters with tools such as CheckM . Expectations from Hi-C binning Hi-C metagenome binning is dependent on both the quality of the shotgun assembly and the depth of true Hi-C read-pairs. For a Hi-C data-set with strong signal (perhaps 10%) and reasonable depth of sequencing (100M pairs), it is reasonable to expect 50-100 nearly complete MAGs [6]. References \u00b6 1. Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science , 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369 fchen. (n.d.). HiCpipe. Retrieved from https://github.com/ChenFengling/HiCpipe Wolff, J., Bhardwaj, V., Nothjunge, S., Richard, G., Renschler, G., Gilsbach, R., \u2026 Gr\u00fcning, B. A. (2018). Galaxy HiCExplorer: a web server for reproducible Hi-C data analysis, quality control and visualization. Nucleic Acids Research , 46(W1), W11\u2013W16. https://doi.org/10.1093/nar/gky504 DeMaere, M. Z., Darling A. E. (n.d.). qc3C. Retrieved from https://github.com/cerebis/qc3C hic_qc. (n.d.). Retrieved from https://github.com/phasegenomics/hic_qc DeMaere, M. Z., & Darling, A. E. (2019). bin3C: exploiting Hi-C sequencing data to accurately resolve metagenome-assembled genomes. Genome Biology , 20(1), 46. https://doi.org/10.1186/s13059-019-1643-1 Press, M. O., Wiser, A. H., Kronenberg, Z. N., Langford, K. W., Shakya, M., Lo, C.-C., \u2026 Liachko, I. (2017). Hi-C deconvolution of a human gut microbiome yields high-quality draft genomes and reveals plasmid-genome interactions (p. 198713). https://doi.org/10.1101/198713","title":"Hi-C"},{"location":"hic/#exploiting-dna-dna-proximity","text":"","title":"Exploiting DNA-DNA Proximity"},{"location":"hic/#what-is-metagenomic-hi-c","text":"As the name suggests, Metagenomic Hi-C is an adaptation of the original Hi-C protocol -- used for the 3-dimensional study of chromosome structure [1] -- to shotgun metagenome analysis. Metagenomic Hi-C is a promising addition to our analysis toolbox, as it makes it possible for genome binning to be carried out accurately and precisely from a single sample rather than requiring large collection of samples (such as in timeseries binning). The Hi-C protocol was designed to capture genome-wide evidence of DNA molecules in close physical proximity in vivo . With the flexible nature of a chromosome and its capacity to bend back on itself, the most frequently observed interactions are usually those between nearby loci on the same chromosome (intra-chromosomal). Though the rate of interaction between loci falls off quickly with separation, it does not go to zero and long-distance contacts within a chromosome remain observable. Less frequent than intra-chromosomal interactions are those between separate molecules (chromosomes, plasmids, etc) within the same cell (inter-chromosomal). To what degree we see inter-chromosomal interactions depends greatly on the organism(s). The least common are inter-cellular interactions and are often seen at rates far below those found within a cell. The proximity information captured in Hi-C sequencing data can be readily applied to improve draft genome and metagenome assemblies. In metagenomics we use the proximity information to infer which assembly fragments (contigs) originated from the same cell (or chromosome). Though the power of the \"proximity signal\" should ultimately make it possible to infer individual genotypes, current tools stop at the same resolution as timeseries binning, associating assembly fragments into MAGs.","title":"What is Metagenomic Hi-C?"},{"location":"hic/#the-hi-c-library-protocol","text":"Major steps of the Hi-C protocol","title":"The Hi-C Library Protocol"},{"location":"hic/#basic-concept","text":"Though Hi-C sequencing data is generated using conventional high-throughput Illumina paired-end sequencing, the more complicated Hi-C protocol produces a very different library. Unlike traditional shotgun sequencing, where read pairs originate from nearby regions on the same contiguous DNA molecule (i.e. chromosome, plasmid, etc), the Hi-C protocol can generate read-pairs from any two strands of DNA that were in close physical proximity within the cell. So long as the two DNA loci interact, regardless of their location with the genome, there is a chance of producing a Hi-C read-pair from that interaction. Hi-C read pairs can therefore originate from far-away regions of the same molecule or from entirely different molecules. To achieve this, the Hi-C library protocol involves several steps upstream of Illumina adapter ligation and is more challenging to execute in the lab. As a result, commercial kits have appeared which aim simplify the process and improve quality and consistency of the resulting libraries.","title":"Basic Concept"},{"location":"hic/#some-commercial-companies-offering-varieties-of-hi-c-kits-and-services","text":"Phase Genomics Arima Genomics Dovetail Genomics","title":"Some commercial companies offering varieties of Hi-C kits and services"},{"location":"hic/#rough-protocol-outline","text":"DNA Fixation: Beginning with intact cells, the first step of the Hi-C protocol is formalin fixation. The act of cross-linking \"locks in\" close-by conformation arrangements within the DNA that existed in the cells at the time of fixation. Cell Lysis: The cells are lysed and DNA-protein complexes extracted and purified. Restriction Digest: The DNA is digested using a restriction endonuclease. In metagenomic Hi-C, enzymes with large overhangs and 4-nt recognition sites are common choices (i.e. Sau3AI, MluCI, DpnII). Differences in the GC bias of the recognition site and the target DNA is an important factor, as inefficient digestion will produce fewer Hi-C read-pairs. Biotin Tagging: The overhangs produced during digestion are end-filled with biotinylated nucleotides. Free-end Ligation: in dilute conditions or immobilised on a substrate, free-ends protruding from the DNA-protein complexes are ligated. This stochastic process favours any two ends which were nearby within the complex, however random ligation and self-ligation can occur and the reads resulting from such events create noise/error in the data. Crosslink Reversal: The bonds created by formalin fixation are removed, allowing the now free DNA to be purified. Proteinase digestion is common following this step. Un-ligated End Clean-up: Free-ends which failed to form ligation products are unwanted in subsequent steps but could still be biotin tagged. To minimise their inclusion, a light exonuclease 3' to 5' favoured chew-back can be applied. DNA Shearing: With ligation completed, the DNA is mechanically sheared and the size range of the resulting fragment selected suitability with Illumina paired-end sequencing. DNA repair: Sonication can lead to damaged ends and nicks, which can be repaired. Proximity Ligation Enrichment: Biotin tagged fragments are pulled down using affinity purification. Adapter Ligation: Illumina paired-end adapter ligation and associated steps to produce a sequencing library are now applied.","title":"Rough Protocol Outline"},{"location":"hic/#quality-control-is-my-library-ok","text":"Due to the highly variable nature of samples taken directly from an environment, the possible presence of enzyme inhibitors, and other reasons, the metagenomic Hi-C protocol may not always work with high efficiency, or at all. Problems with sample processing can sometimes be identified prior to sequencing, for example if the measured library yield is very low. But other problems may be difficult to diagnose prior to sequencing, for example, it is very difficult to estimate the efficiency of proximity ligation and thus the fraction of sequencing reads that will be Hi-C reads without actually sequencing the sample. The reads that are not Hi-C reads are essentially conventional shotgun read-pairs, which in the context of Hi-C are uninformative. The value of the proximity information contained within a Hi-C read-set is high enough that even with a low efficiency library, it can be worthwhile to compensate for low efficiency by sequencing more deeply to obtain more Hi-C read pairs. Thus, determining the percentage of Hi-C read-pairs in the library is important, because it can tell us how deeply we need to sequence our library. Rather than submit a library to a large and costly sequencing run immediately, a small pilot sequencing run can be sufficient to confidently estimate the Hi-C efficiency. Armed with this information, a researcher can make informed decisions about further action; such as whether a candidate Hi-C library should be fully sequenced and to what depth. We now discuss various ways to estimate the fraction of Hi-C reads in a library.","title":"Quality Control: is my library ok?"},{"location":"hic/#evidence-of-proximity-ligation","text":"","title":"Evidence of Proximity Ligation"},{"location":"hic/#long-range-pairs","text":"The separation distance for intra-chromosomal Hi-C read-pairs is bounded only by the length of the chromosome. This is unlike shotgun read-pairs whose separation is rarely more than a thousand nucleotides due to chemistry limitations on Illumina instruments. Though it is less direct than looking for evidence of cut-site duplication events, a simple count of the number of read pairs than map far apart can be used to infer the percentage of Hi-C read-pairs. The main limitation of this approach is that it requires a high quality reference assembly against which the Hi-C read pairs can be mapped. In many metagenomic projects, such an assembly is not actually available, either because a good quality shotgun metagenome dataset has not yet been generated for the sample or because there are no close reference genomes in public databases for the organisms in the sample. Counting the number of of pairs which map at long-range can as be used to infer the percentage of Hi-C read-pairs.","title":"Long-range pairs"},{"location":"hic/#proximity-ligation-junctions","text":"In the Hi-C protocol outlined above, the steps of creating free-ends through enzymatic digestion, subsequent end-repair, and finally re-ligation introduces an artifact at the junction point. This artifact is a short sequence duplication and the exact sequence enzyme dependent. The duplication is produced when the cleavage site overhangs are subjected to end-repair. When the repaired blunt ends are subsequently ligated, their junction contains two copies of the overhang sequence. Proximity junction for the enzyme Sau3AI Sau3AI is a 4-cutter with a 4nt overhang. Native DNA 5`-XXXXGATCYYYY-3' 3'-xxxxCTAGyyyy-5' Free-ends post cleavage have overhangs 5`-XXXX GATCYYYY-3' 3'-xxxxCTAG yyyy-5' Blunt ends after end-repair with biotinylated nucleotides 5`-XXXXGATC GATCYYYY-3' 3'-xxxxCTAG CTAGyyyy-5' Free-end ligation produces a duplication at the junction 5`-XXXXGATCGATCYYYY-3' 3'-xxxxCTAGCTAGyyyy-5' After the steps of DNA shearing and size selection, fragments which eventually go on to DNA sequencing can contain a junction at any point along their extent. An Illumina paired-end sequencing run, which generates reads at either end of a fragment, thus has two chances to read through the junction. This approach has two main drawbacks: If the fragments in the sequencing library are long relative to the read length, there may be junctions within the fragment that remain unobserved, and this fact must be corrected for in the estimate. If the sample was degraded there may have been free ends that were not created by an enzyme cut. These could become proximity-ligated, and therefore ligation junctions may exist that do not contain the obvious junction sequence. Searching the read-set for examples of this junction sequence is one means of measuring the percentage of Hi-C read-pairs in a given library.","title":"Proximity ligation junctions"},{"location":"hic/#using-a-qc-tool","text":"Though the methodology of Hi-C QC has yet to achieve standardisation, a number of tools exist which offer forms of QC testing. These tools can be found as components of Hi-C analysis pipelines ( HiCpipe ), embedded in tool-suites ( HiCExplorer ) and as stand-alone command-line tools ( hic_qc , qc3C ) [2-5]. In this tutorial we will use qc3C to assess Hi-C library quality in two different ways: Read mapping based analysis k -mer based analysis.","title":"Using a QC Tool"},{"location":"hic/#get-some-hi-c-data-and-tools","text":"For a single timepoint, we'll first download a small Hi-C read-set and its associated shotgun read-set. Also, we'll pull down the Docker images for qc3C (quality testing) and bin3C (metagenome binning) Get some Hi-C data and the qc3C and bin3C docker images # download the read-set parallel-fastq-dump/parallel-fastq-dump -s SRR9323809 -s SRR8960211 --threads 4 --outdir hic_data/ \\ --minSpotId 0 --maxSpotId 1000000 --split-files --gzip # fetch the qc3C image docker pull cerebis/qc3c:alpine # fetch the bin3c image docker pull cerebis/bin3c:latest","title":"Get some Hi-C data and tools"},{"location":"hic/#create-a-metagenome-assembly-and-map-hi-c-reads","text":"The shotgun dataset has been limited to 1M read-pairs, this is only 1/165 th of the total. Obviously this reduction in total coverage will lead to a much sparser sampling of the community and a more fragmented assembly. Nevertheless, it is enough data to demonstrate the concept of QC and Hi-C metagenome binning. Lets assemble the shotgun data and map both library types to the resulting contigs. Create a SPAdes assembly # enter the hic data directory cd hic_data # launch spades in metagenomic mode docker run -v $PWD :/opt/app-root cerebis/bin3c:latest spades.py --meta -1 SRR8960211_1.fastq.gz -2 SRR8960211_2.fastq.gz -o asm Once we have our assembly, we can map the Hi-C reads to the contigs. This mapping forms the basis of what binning tools like bin3C use to infer proximity associations between DNA loci. Map Hi-C reads to assembly contigs # index the contig fasta docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bwa index asm/contigs.fasta # map hi-c reads to the contigs docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR9323809_1.fastq.gz SRR9323809_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o hic_to_ctg.bam -\" # map shotgun reads to the contigs docker run -v $PWD :/opt/app-root cerebis/bin3c:latest /bin/bash -c \"bwa mem -t4 -5SP asm/contigs.fasta SRR8960211_1.fastq.gz SRR8960211_2.fastq.gz | samtools view -uS -F 0x904 - | samtools sort -@4 -n -o wgs_to_ctg.bam -\" # return to your home directory cd Breaking down the read mapping command We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/bin3c:latest ). Finally we include the actual call, which involves pipes between bwa and samtools . To achieve this we pass the bash shell our complex command, which it then executes within the container.","title":"Create a metagenome assembly and map Hi-C reads"},{"location":"hic/#bam-mode-analysis-with-qc3c","text":"We begin with the conceptually simpler approach to Hi-C QC, counting the number of long-range read-pairs from a Hi-C read-set. As a rule of thumb, to work well with Illumina sequencing, the physical size of fragments is likely to be <1000nt. We shall therefore simply count the number of pairs which map with a separation >1000, >5000 and >10,000 nt. As the mapped read-pair separation grows beyond 1000nt we expect an increasingly large proportion of these pairs will be due to Hi-C proximity ligation. Conversely, when analysing a shotgun read-set, we expect to see very few. Run a BAM based analysis # enter the hic data folder cd hic_data # perform bam based QC analysis on Hi-C data docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b hic_to_ctg.bam\" # now perform the analysis on shotgun data docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C bam -m 400 -e Sau3AI -e MluCI -b wgs_to_ctg.bam\" # return to your home directory cd","title":"BAM mode analysis with qc3C"},{"location":"hic/#estimation-of-fragment-size","text":"As we are not sure of the mean insert length, we guess it to be 400nt. This is one handy benefit of the BAM mode approach, we can infer a reasonably good estimate of the fragment (insert) length. In k -mer mode, having an accurate value becomes important if we wish to estimate the probable fraction of unobserved proximity junctions. That is to say, for long fragments and short reads, much of each fragment goes unsequenced and so we will miss the evidence of junctions. In our resulting log from above, qc3C reports an observed mean fragment length of 445nt for the Hi-C read-set. INFO | 2019-06-23 18:55:18,652 | qc3C.bam_based | Observed mean of short-range pair separation: 445nt","title":"Estimation of fragment size"},{"location":"hic/#looking-at-the-results","text":"The last three lines of the output from qc3C report the absolute number and relative fraction of read-pairs that mapped far apart. For the Hi-C library, the fraction of pairs separated by more than 10kb was ~2.6%, while in contrast for the known shotgun library the fraction was <0.006%. Hi-C result INFO | 2019-11-19 08:31:13,975 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-11-19 08:31:13,975 | qc3C.bam_based | Number of observed pairs: 366, 169, 84 INFO | 2019-11-19 08:31:13,975 | qc3C.bam_based | Relative fraction of all cis: 0.1149, 0.05308, 0.02638 INFO | 2019-11-19 08:31:13,976 | qc3C.bam_based | Relative fraction of all pairs: 0.04934, 0.02278, 0.01132 Shotgun result INFO | 2019-11-19 08:31:42,171 | qc3C.bam_based | Long-range distance intervals: 1000nt, 5000nt, 10000nt INFO | 2019-11-19 08:31:42,171 | qc3C.bam_based | Number of observed pairs: 284, 43, 23 INFO | 2019-11-19 08:31:42,172 | qc3C.bam_based | Relative fraction of all cis: 0.0007268, 0.0001101, 5.886e-05 INFO | 2019-11-19 08:31:42,172 | qc3C.bam_based | Relative fraction of all pairs: 0.0003572, 5.408e-05, 2.893e-05 It is important to keep in mind that as the assembly was made from a shotgun dataset downsampled to very low coverage, the length of contigs is significantly lower than would have been obtained with the full dataset. This will impair the odds of seeing pairs which map far away. Despite this, the 98% of pairs in the shotgun library map to the same contig sequence. In contrast, 73% of Hi-C pairs mapped to the same contig. Overall, for BAM mode, the reliability of results is connected to the quality and completeness of the available references. Breaking down the invocation of qc3C We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to qc3C , along with its own options. Potential Concerns Requires a reference sequence: a completed genome a draft assembly. May not be an option when: no reference is available only a very highly fragmented draft exists Computational requirements: create the shotgun assembly map Hi-C reads to the reference The distribution of assembly contig lengths will affect the maximum mapping distance.","title":"Looking at the results"},{"location":"hic/#k-mer-mode-analysis-with-qc3c","text":"To analyze our small Hi-C read-set using a k -mer approach, we will first create a k -mer library using Jellyfish. Since this Hi-C experiment used the 4-cutter Sau3AI, which produces an 8-mer junction, we'll use a mer size of 24 for the library. This will give us 8 nucleotides either side of any prospective junction, for specificity. Choosing how large a k -mer size to use for the library is a trade-off between computational complexity and minimising false discovery. You can try sizes bigger and smaller. Running k-mer based analyses On the Hi-C data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files rm -f gen_fq && for fn in ` ls SRR9323809*fastq.gz ` ; do echo gzip -dc $fn >> gen_fq ; done # use jellyfish to create a 24-mer library from FastQ reads docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_fq -o hic24.jf # perform k-mer based QC analysis docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 445 -s 1234 -e Sau3AI -l hic24.jf -r SRR9323809_1.fastq.gz -r SRR9323809_2.fastq.gz\" # return back to your home directory cd Now on the shotgun data # enter the hic data folder cd hic_data # create a generator used by Jellyfish for compressed files echo \"gzip -dc SRR8960211_1.fastq.gz\" > gen_sg # use jellyfish to create a 24-mer library from FastQ reads docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine jellyfish count -t 4 -m 24 -s 100M -C -g gen_sg -o wgs24.jf # perform k-mer based QC analysis docker run -v $PWD :/opt/app-root cerebis/qc3c:alpine ash -c \"qc3C kmer -m 306 -s 1234 -e Sau3AI -l wgs24.jf -r SRR8960211_1.fastq.gz -p 0.05\" # return back to your home directory cd","title":"k-mer mode analysis with qc3C"},{"location":"hic/#looking-at-the-results_1","text":"Looking at the inferred fraction of true Hi-C read-pairs, we can immediately see that the low coverage shotgun assembly appears to have drastically skewed the estimate of signal (2.7% vs 5.8%). When estimating Hi-C content using an assembly based method, we require a decent assembly. After adjustment for the unobserved fraction (0.32), the k-mer based estimate of signal is increased to 7.6%. In contrast, for the conventional shotgun data-set we should expect an estimate near zero and we see a fairly low value of 0.094%. This tells us that were we to see such low values for a Hi-C library, something has gone seriously wrong with the library prep and further sequencing would be best abandoned. As another check, if we look further up the log we can see that the number of observed junctions was 0.098%, while we could expect 0.0015% purely by random chance. Although this difference might appear significant, it is put clear in perspective when compared to the Hi-C result, where 5.9% of reads contained the junction sequence. This Hi-C library appears to have a reasonable fraction of Hi-C reads. If it were a pilot sequencing run, this result would be sufficient evidence to commit to a larger sequencing run. Hi-C result INFO | 2019-11-19 08:35:53,320 | qc3C.kmer_based | Number of unique observations used in empirical p-value: wgs 348,653, hic 21,592 INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 5.753 \u00b1 0.004587 % INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | Observed mean read length for paired reads: 151nt INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | For supplied insert length of 445nt, estimated unobserved fraction: 0.3213 INFO | 2019-11-19 08:35:53,470 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 7.601 \u00b1 0.006062 % Shotgun result INFO | 2019-11-19 08:37:46,411 | qc3C.kmer_based | Number of unique observations used in empirical p-value: wgs 49,715, hic 49 INFO | 2019-11-19 08:37:46,433 | qc3C.kmer_based | Estimated Hi-C read fraction via p-value sum method: 0.09371 \u00b1 0.003008 % INFO | 2019-11-19 08:37:46,434 | qc3C.kmer_based | Observed mean read length for paired reads: 151nt INFO | 2019-11-19 08:37:46,434 | qc3C.kmer_based | For supplied insert length of 306nt, estimated unobserved fraction: 0.01307 INFO | 2019-11-19 08:37:46,434 | qc3C.kmer_based | Adjusted estimation of Hi-C read fraction: 0.09493 \u00b1 0.003048 % Breaking down the invocation of jellyfish We are providing Docker access to our host filesystem using a bind mount ( -v $PWD:/opt/app-root ) Next we are specifying the image to run ( cerebis/qc3c:alpine ). Finally we include the actual call to jellyfish , along with its own options. K-mer mode requirements No-assembly or reference required Requires making a k-mer library from FastQ reads Analyse FastQ reads against library","title":"Looking at the results"},{"location":"hic/#metagenome-binning-with-hi-c-data","text":"As we saw by processing timeseries data with MetaBAT2 , metagenome binning can also be performed using Hi-C. The primary difference here is that accurate and precise binning is possible from a single timepoint, so long as you have you have both a shotgun and Hi-C read-set. Currenty, only a few tools exist which were made specifically for this purpose ( bin3C , ProxiMeta ) [6,7]. In this tutorial we will use bin3C, as it is open-source and easy to use. With a set of reference sequences and Hi-C to reference BAM file in hand (from above), a bin3C analysis has two stages: first create the contact map, second cluster the contact map. We also need to know what enzyme(s) was used in generating the library. It is becoming more common to generate libraries from two enzymes with differing GC biases to improve Hi-C coverage within a community. In such cases, we simply specify them when creating the map, as here. Perform Hi-C metagenome binning # enter hic data directory cd hic_data # Make contact map docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C mkmap -v --clobber -e Sau3AI -e MluCI --eta --bin-size 5000 asm/contigs.fasta hic_to_ctg.bam bin3C_out # Cluster contact map docker run -v $PWD :/opt/app-root cerebis/bin3c:latest bin3C cluster -v --clobber --only-large bin3C_out/contact_map.p.gz bin3C_out # return to your home directory cd","title":"Metagenome Binning with Hi-C Data"},{"location":"hic/#looking-at-the-results_2","text":"Our example data-set was insufficient Unfortunately, the example data we have used here is insufficient to get a useful result from bin3C. Neither the shotgun, nor the Hi-C data-set was sequenced deeply enough. In reality, we artificially limited the depth when downloading the FastQ data with --max-spot . Were we to go back and obtain the full data-sets, we would achieve a much better result (see the figure below). After bin3C completes the clustering step, the three outputs which are possibly the most interesting are: The comma delimited cluster_report.csv table the cluster_report details characteristics of each identified cluster. An id, name, number of contigs, total extent, mean GC and coverage, etc. Mean coverage is a reasonable gauge of relative abundance A qualitative overview of the result in the form of a heatmap cluster_plot.png contigs are organized by cluster, followed by descending length a pixel represents a 5kb bin and therefore contigs are represented proportional to their length. Clustered sequence data the fasta/ directory contains cluster FastA data The cluster plot is a visual representation of the clustered contact map. As a metagenomic assembly can easily contain 50k to 100k contigs, which as a NxN image would be very large, the plot's resolution is constrained by default to 4000x4000 pixels to conserve memory. Once organised, a good Hi-C read-set will have produced a map that is clearly in block-diagonal form with high contrast. Also, a significant proportion of the map's field is should be dark, reflecting the fact that inter-cellular interactions are rare. In cases where there was insufficient signal, there will only be a few (if any) large clusters and they will look sparse internally. Example cluster plot of a human microbome Right click to get a larger image MAG clusters of a single-timepoint human faecal microbiome (PRJNA413092) as determined by bin3C . The shotgun read-set consisted of 249M pairs, while the Hi-C library contained 41M pairs (~10% Hi-C). Of the identified MAGs, 55 where >90% complete, <5% contamination as estimated by CheckM [6]. If you are familiar with the appearance of a single-genome microbial contact map, at sufficient zoom you may notice that the individual clusters do not look the same. This is due to the fact that the proper ordering of contigs (scaffolding) within the clusters has not been established. After reaching this point, standard downstream analyses would be to profile the taxonomic composition of the clusters with tools such as CheckM . Expectations from Hi-C binning Hi-C metagenome binning is dependent on both the quality of the shotgun assembly and the depth of true Hi-C read-pairs. For a Hi-C data-set with strong signal (perhaps 10%) and reasonable depth of sequencing (100M pairs), it is reasonable to expect 50-100 nearly complete MAGs [6].","title":"Looking at the results"},{"location":"hic/#references","text":"1. Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev, M., Ragoczy, T., Telling, A., \u2026 Dekker, J. (2009). Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science , 326(5950), 289\u2013293. https://doi.org/10.1126/science.1181369 fchen. (n.d.). HiCpipe. Retrieved from https://github.com/ChenFengling/HiCpipe Wolff, J., Bhardwaj, V., Nothjunge, S., Richard, G., Renschler, G., Gilsbach, R., \u2026 Gr\u00fcning, B. A. (2018). Galaxy HiCExplorer: a web server for reproducible Hi-C data analysis, quality control and visualization. Nucleic Acids Research , 46(W1), W11\u2013W16. https://doi.org/10.1093/nar/gky504 DeMaere, M. Z., Darling A. E. (n.d.). qc3C. Retrieved from https://github.com/cerebis/qc3C hic_qc. (n.d.). Retrieved from https://github.com/phasegenomics/hic_qc DeMaere, M. Z., & Darling, A. E. (2019). bin3C: exploiting Hi-C sequencing data to accurately resolve metagenome-assembled genomes. Genome Biology , 20(1), 46. https://doi.org/10.1186/s13059-019-1643-1 Press, M. O., Wiser, A. H., Kronenberg, Z. N., Langford, K. W., Shakya, M., Lo, C.-C., \u2026 Liachko, I. (2017). Hi-C deconvolution of a human gut microbiome yields high-quality draft genomes and reveals plasmid-genome interactions (p. 198713). https://doi.org/10.1101/198713","title":"References"},{"location":"qc/","text":"Sequencing run QC \u00b6 Get some sequence data \u00b6 The first thing we'll do is to get some sequence data to work with. If you are working on a new sequencing project the data might instead come directly from a sequencing facility. For this tutorial we'll work with data that is available in public databases. Published sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ. Each of these databases provide convenient search interfaces , which can be used to find samples by keyword and sample type (e.g. shotgun metagenome). The data-sets we'll use in the following exercises (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) are small in size and therefore quick to process. With an accession number in hand, the easiest way to obtain the related sequencing data from the databases is via the fastq-dump software. First, let's create a new conda environment, install Python along with the SRA Toolkit and the activate it. To do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (its ok to copy and paste): Setup an initial set of tools # fast download from github git clone https://github.com/rvalieris/parallel-fastq-dump.git # create a new conda environment and activate it conda create -y -n workshop \"python<3\" sra-tools conda activate workshop Info We've just created a new conda environment and activated it. You will see your prompt now tells you this fact with (workshop) . Now that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal: Download the sequencing data for our data-sets parallel-fastq-dump/parallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 \\ --threads 4 --split-files --gzip --outdir qc_data/ If the download was successful, a new subfolder qc_data should exist and within that you should find the fastq files. Using your terminal session, try listing the contents of the subfolder ( ls qc_data ). You should see something like the following: You can check that the fastq files exist Evaluating sequence quality with FastQC and MultiQC \u00b6 The very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data. There are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install fastqc and multiqc . Install fastqc and multiqc for QC analysis conda install -y -n workshop fastqc multiqc In the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called pip . pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster. Use fastqc to analyse the datasets cd qc_data ls *.fastq.gz | xargs fastqc Unpacking the commands First we change the current directory ( cd qc_data ). any subsequent command will, by default, now run in qc_data. Next we get the list of fastq files ( ls *.fastq.gz ) and pass that to xargs . the wildcard expression ( *.fastq.gz ) matches all of the sequencing data files. this list of files is then piped ( | ) to the next command xargs . the xargs command allows you to build and execute command lines. In this simplest form, xargs merely takes the list of files it receives and makes them arguments to fastqc . fastqc is then run, where each of the fastq files will included as an argument. If this step has worked, then you should have several new .zip files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc. Create a summary report multiqc --title \"Raw Reads\" . At this point a multiqc report file will appear inside the QC directory. First double click to open the QC folder. Opening our working directory in Jupyter Once we've navigated to the QC directory, a file called Raw-Reads_multiqc_report.html will appear in the listing. To open this report, simply double-click on the file name and a new tab will appear in Jupyter. Opening the HTML report in Jupyter Switching to the report tab, first click \"Trust HTML\" button in the top left-hand corner. Doing this will permit JavaScript to execute within the page, which is used for drawing the figures. Opening the HTML report in Jupyter From here we can evaluate the quality of the libraries. Cleaning up reads with fastp \u00b6 Now that we have evaluated the condition of our raw data-sets, we are in a better position to judge whether everything looks ok. Though we may feel that our data is in an acceptable state for further analysis, sometimes smaller details can escape us. Therefore, subjecting data-sets to a experimentally suitable clean-up procedure is good practice. What is suitable for a given experiment depends on what will ultimately be inferred. In the case of assembly, contaminating sequence or an abundance of low-quality base-calls will likely lead to worse results. Therefore, identifying and removing this type of error will be advantageous. In constrast, clean-up procedures the attempt to correct base-calling errors can be inappropriate when a sample is not clonal (ie. metagenomics). Here, the assumption that SNV are errors, leads to software removing the micro-diversity within the sample. While sequencing technology continues to evolve and refine, data-sets generated from even the latest NGS platforms will not be error-free. There exist both random and systematic sources of error in sequencing data and a wide assortment of tools are available to identify and eliminate them. It is important to note that not every clean-up procedure is appropriate for all types of experiment and care should be taken not to falsely remove true observations from your data. For our shotgun sequencing data, we will use fastp , which runs quickly and is easy to use. Install fastp for cleaning up reads conda install -y -n workshop fastp By default, fastp will apply the following clean-ups: automatically identify and remove lingering adapter sequence. trim off low quality bases on the tail-end. filter out very short reads and reads with too many Ns. Lingering adapter sequence can be thought of an experimental contamination and represents a misleading source of systematic error, while low quality base calls are more-so a source of random error. Run fastp on our data-sets mkdir cleaned for r1 in $( ls *_1.fastq.gz ) do # extract the SRR from the filename srr = $( basename $r1 _1.fastq.gz ) # run fastp on paired reads fastp --in1 ${ srr } _1.fastq.gz --in2 ${ srr } _2.fastq.gz \\ --out1 cleaned/ ${ srr } _1.fastq.gz --out2 cleaned/ ${ srr } _2.fastq.gz done Unpacking the commands create a new subfolder for holding the cleaned data-sets ( mkdir cleaned ) using a for-loop in Bash, individually submit R1/R2 pairs to fastp the loop iterates over the list of just R1 files, obtained using the wildcard *_1.fastq.gz for each R1 file, extract the base SRR id ( srr=$(basename $r1 _1.fastq.gz) ) call fastp for paired R1/R2 files, constructing from the srr: the input files ( ${srr}_1.fastq.gz , ${srr}_2.fastq.gz ) the output files ( cleaned/${srr}_1.fastq.gz , cleaned/${srr}_2.fastq.gz ) Although fastp produces its own reports, we will again use fastqc and multiqc so that we can compare our cleaned and raw data-sets. Perform QC analysis on the cleaned data-sets ls cleaned/*.fastq.gz | xargs fastqc multiqc --title \"Cleaned Reads\" cleaned The resulting multiqc report will be entitled Cleaned-Reads_multiqc_report.html . Double click this file in Jupyter to open the tab and select \"Trust HTML\" to enable the JavaScript figures as described above for the raw data-sets. As none of our data-sets suffers from serious problems, when we compare the Raw and Cleaned reports, there are not many obvious changes. One difference that is easily noticeable is found in the section \" Adapter Content \". Although only a small proportion of reads were affected by lingering adapter sequence in the raw data-sets, fastp has removed what remained. Without too much effort, we have eliminated a source of systematic error -- tools downstream will no longer be tasked with reconciling an artifact of data collection with real observations. Taxonomic analysis \u00b6 Metagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data. It can give us a very high-level, rough idea of what kinds of microbes are present in a sample. It can also give an idea of how complex/diverse the microbial community is -- whether there are many species or few. It is useful as an initial quality check to ensure that the microbial community composition looks roughly as expected, and to confirm that nothing obvious went wrong during the sample collection and sequencing steps. Taxonomic analysis with Metaphlan2 \u00b6 While it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\". Therefore it's suggested to install it via the simple download method described on the metaphlan tutorial page : Skip downloading metaphlan2 The >500MB Metaphlan2 package has already been downloaded and we'll just be extracting it. Obtaining metaphlan # this would be how you could download the package yourself # wget -c -O metaphlan.tar.bz2 https://bitbucket.org/nsegata/metaphlan/get/default.tar.bz2 # extract the archive cd ; tar xvjf /data/metaphlan.tar.bz2 # rename the folder to something simple mv nsegata-metaphlan* metaphlan # apply a technical fix for metaphlan plotting sed -i 's/axisbg/facecolor/' metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py # install dependencies conda install -y -n workshop bowtie2 numpy scipy matplotlib Once metaphlan has been prepared we can run it on our QC samples: Running metaphlan on our data cd ~/qc_data for pig in SRR9323808 SRR9323810 SRR9323811 SRR9323809 do zcat ${ pig } *.fastq.gz | ~/metaphlan/metaphlan.py --input_type multifastq --bt2_ps very-sensitive \\ --bowtie2db ~/metaphlan/bowtie2db/mpa --bowtie2out ${ pig } .bt2out -o ${ pig } .mph2 done Unpacking the commands The above series of commands is another exampe of a Bash \"for loop\", where each iteration processes one of our metagenome samples. As before, this is a convenient and less error-prone way to process many samples by avoiding the need to type out the commands for each sample. At each loop iteration, one of the sample names is placed into the loop variable ${pig} , where it can get used in the command inside the loop. Secondly, we decompress ( zcat ) and then feed each pig's sequencing data to metaphlan through a pipe ( | ) Finally we can plot the taxonomic profile of the samples: Plot the taxonomic profile of each sample ~/metaphlan/utils/merge_metaphlan_tables.py *.mph2 > pig_mph2.merged ~/metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py -c bbcry --top 25 --minv 0.1 -s log --in pig_mph2.merged --out mph2_heatmap.png Once that has completed successfully, a new file called mph2_heatmap.png will appear in the qc_data folder of our Jupyter file browser. We can double click it to view. There are other ways to visualize the data, and they are described in the graphlan section of the metaphlan tutorial page. Taxonomic analysis with other tools \u00b6 There are a whole range of other software tools available for metagenome taxonomic analysis. They all have strengths and weaknesses. A few other commonly used tools are listed here: kraken2 MEGAN Centrifuge CLARK Evaluating the host genomic content \u00b6 In many applications of metagenomics we are working with host-associated samples. The samples might have come from an animal gut, mouth, or other surface. Similarly for plants we might be working with leaf or root surfaces or rhizobia. When such samples are collected the resulting DNA extracts can include a significant fraction of host material. Let's have a look at what this looks like in data. To do so, we'll download another set of pig gut samples: a set of samples that was taken using scrapings or swabs from different parts of the porcine digestive system, including the duodenum, jejunum, ileum, colon, and caecum. Rather than using metaphlan2 to profile these, we will use the kraken2 classifier in conjunction with the bracken tool for estimating relative abundances. We first need to install kraken2 and bracken : Install kraken2 and braken conda install -y -n workshop kraken2 bracken Next, we need a kraken2 database. For this tutorial we will simply use a precomputed kraken2 database, note however, the very important limitation that the only non-microbial genome it includes is the human genome. If you would like to evaluate host genomic content on plants or other things, you should follow the instructions on the kraken2 web site to build a complete database. Skip downloading the database The Kraken team maintain this and a number of other prepared databases which they make available for download. However, as a matter of courtesy, we will avoid repeatedly requesting the same data. The archive has already been downloaded and we will just be extracting the contents as follows: Extract the precomputed kraken2 reference database cd ; tar xvzf /data/minikraken2_v2_8GB_201904_UPDATE.tgz Finally we are ready to profile our samples with kraken2 and bracken . We'll first download the sample set with parallel-fastq-dump and then run kraken2 and bracken 's est_abundance.py script on each sample using a bash for loop. The analysis can be run as follows: Download some new samples # the list of samples samples = \"SRR9332442 SRR9332438 SRR9332439 SRR9332443 SRR9332440\" # download the associated data parallel-fastq-dump/parallel-fastq-dump ${ samples //SRR/-s SRR } --threads 4 --split-files \\ --gzip --outdir host_qc/ # switch to the data directory and analyse cd host_qc for s in $samples do # run kraken2 analysis kraken2 --paired ${ s } _1.fastq.gz ${ s } _2.fastq.gz --db ~/minikraken2_v2_8GB_201904_UPDATE/ \\ --report ${ s } .kreport > ${ s } .kraken # estimate abundance with braken est_abundance.py -i ${ s } .kreport -o ${ s } .bracken \\ -k ~/minikraken2_v2_8GB_201904_UPDATE/database150mers.kmer_distrib done Once the above has completed, navigate over to the host_qc folder in the Jupyter file browser and click on the *.bracken files to open them. What do you see? \u00b6 In particular, why does sample SRR9332438 look so different to sample SRR9332440? Keep in mind the isolation sources of the samples were as follows: Sample Source SRR9332442 Duodenum SRR9332440 Caecum SRR9332439 Ileum SRR9332438 Jejunum SRR9332443 Colon Some challenge questions \u00b6 If we sequenced samples from pigs, why is human DNA being predicted in these samples? If we were to design a large study around these samples which of them would be suitable for metagenomics, and why? How much sequencing data would we need to generate from sample SRR9332440 to reconstruct the genome of the Bifidobacterium in that sample? What about the E. coli ? Are there really six species of Lactobacillus present in SRR9332440? Go to the NCBI SRA search tool and find a metagenome of interest to you. Download the first 100000 reads from it (use the --maxSpotId parameter) and analyse it with kraken2. Is it what you expected? How does it compare to the others we've looked at? A note on negative controls \u00b6 Negative controls are a key element in any microbiome profiling or metagenome analysis work. Every aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA. This is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab. It is well known that molecular biology reagents frequently contain contaminating DNA. Usually it is at low levels, but this is not always the case. Therefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing. These negative controls can then be used to correct for contamination artifacts in the remaining samples.","title":"Quality Control"},{"location":"qc/#sequencing-run-qc","text":"","title":"Sequencing run QC"},{"location":"qc/#get-some-sequence-data","text":"The first thing we'll do is to get some sequence data to work with. If you are working on a new sequencing project the data might instead come directly from a sequencing facility. For this tutorial we'll work with data that is available in public databases. Published sequence data is usually archived in the NCBI SRA, the ENA, and the DDBJ. Each of these databases provide convenient search interfaces , which can be used to find samples by keyword and sample type (e.g. shotgun metagenome). The data-sets we'll use in the following exercises (SRA accessions SRR9323808, SRR9323810, SRR9323811, SRR9323809) are small in size and therefore quick to process. With an accession number in hand, the easiest way to obtain the related sequencing data from the databases is via the fastq-dump software. First, let's create a new conda environment, install Python along with the SRA Toolkit and the activate it. To do this, start a terminal session in your Jupyter server (click the Terminal icon) and run the following command (its ok to copy and paste): Setup an initial set of tools # fast download from github git clone https://github.com/rvalieris/parallel-fastq-dump.git # create a new conda environment and activate it conda create -y -n workshop \"python<3\" sra-tools conda activate workshop Info We've just created a new conda environment and activated it. You will see your prompt now tells you this fact with (workshop) . Now that you've installed fastq-dump we can use it to download data by accession number. Copy and paste the following to your terminal: Download the sequencing data for our data-sets parallel-fastq-dump/parallel-fastq-dump -s SRR9323808 -s SRR9323810 -s SRR9323811 -s SRR9323809 \\ --threads 4 --split-files --gzip --outdir qc_data/ If the download was successful, a new subfolder qc_data should exist and within that you should find the fastq files. Using your terminal session, try listing the contents of the subfolder ( ls qc_data ). You should see something like the following: You can check that the fastq files exist","title":"Get some sequence data"},{"location":"qc/#evaluating-sequence-quality-with-fastqc-and-multiqc","text":"The very first thing one would normally do when working with a new dataset is to look at some basic quality statistics on the data. There are many, many tools available to compute quality metrics. For our current data we will use the FastQC software, applied to each sample, and then combine the results using MultiQC to a single report. First step is to install fastqc and multiqc . Install fastqc and multiqc for QC analysis conda install -y -n workshop fastqc multiqc In the above we've used conda to install fastqc, but we've used another way to install multiqc -- something called pip . pip is an installer for python programs, and like conda it will download and install the software along with any dependencies. The reason we use pip in this case is because conda can be very, very slow to install some programs and in this case pip is much faster. Use fastqc to analyse the datasets cd qc_data ls *.fastq.gz | xargs fastqc Unpacking the commands First we change the current directory ( cd qc_data ). any subsequent command will, by default, now run in qc_data. Next we get the list of fastq files ( ls *.fastq.gz ) and pass that to xargs . the wildcard expression ( *.fastq.gz ) matches all of the sequencing data files. this list of files is then piped ( | ) to the next command xargs . the xargs command allows you to build and execute command lines. In this simplest form, xargs merely takes the list of files it receives and makes them arguments to fastqc . fastqc is then run, where each of the fastq files will included as an argument. If this step has worked, then you should have several new .zip files containing the QC data in that directory, along with some html files. When we have a lot of samples it is too tedious to look at all the QC reports individually for each sample, so we can summarize them using multiqc. Create a summary report multiqc --title \"Raw Reads\" . At this point a multiqc report file will appear inside the QC directory. First double click to open the QC folder. Opening our working directory in Jupyter Once we've navigated to the QC directory, a file called Raw-Reads_multiqc_report.html will appear in the listing. To open this report, simply double-click on the file name and a new tab will appear in Jupyter. Opening the HTML report in Jupyter Switching to the report tab, first click \"Trust HTML\" button in the top left-hand corner. Doing this will permit JavaScript to execute within the page, which is used for drawing the figures. Opening the HTML report in Jupyter From here we can evaluate the quality of the libraries.","title":"Evaluating sequence quality with FastQC and MultiQC"},{"location":"qc/#cleaning-up-reads-with-fastp","text":"Now that we have evaluated the condition of our raw data-sets, we are in a better position to judge whether everything looks ok. Though we may feel that our data is in an acceptable state for further analysis, sometimes smaller details can escape us. Therefore, subjecting data-sets to a experimentally suitable clean-up procedure is good practice. What is suitable for a given experiment depends on what will ultimately be inferred. In the case of assembly, contaminating sequence or an abundance of low-quality base-calls will likely lead to worse results. Therefore, identifying and removing this type of error will be advantageous. In constrast, clean-up procedures the attempt to correct base-calling errors can be inappropriate when a sample is not clonal (ie. metagenomics). Here, the assumption that SNV are errors, leads to software removing the micro-diversity within the sample. While sequencing technology continues to evolve and refine, data-sets generated from even the latest NGS platforms will not be error-free. There exist both random and systematic sources of error in sequencing data and a wide assortment of tools are available to identify and eliminate them. It is important to note that not every clean-up procedure is appropriate for all types of experiment and care should be taken not to falsely remove true observations from your data. For our shotgun sequencing data, we will use fastp , which runs quickly and is easy to use. Install fastp for cleaning up reads conda install -y -n workshop fastp By default, fastp will apply the following clean-ups: automatically identify and remove lingering adapter sequence. trim off low quality bases on the tail-end. filter out very short reads and reads with too many Ns. Lingering adapter sequence can be thought of an experimental contamination and represents a misleading source of systematic error, while low quality base calls are more-so a source of random error. Run fastp on our data-sets mkdir cleaned for r1 in $( ls *_1.fastq.gz ) do # extract the SRR from the filename srr = $( basename $r1 _1.fastq.gz ) # run fastp on paired reads fastp --in1 ${ srr } _1.fastq.gz --in2 ${ srr } _2.fastq.gz \\ --out1 cleaned/ ${ srr } _1.fastq.gz --out2 cleaned/ ${ srr } _2.fastq.gz done Unpacking the commands create a new subfolder for holding the cleaned data-sets ( mkdir cleaned ) using a for-loop in Bash, individually submit R1/R2 pairs to fastp the loop iterates over the list of just R1 files, obtained using the wildcard *_1.fastq.gz for each R1 file, extract the base SRR id ( srr=$(basename $r1 _1.fastq.gz) ) call fastp for paired R1/R2 files, constructing from the srr: the input files ( ${srr}_1.fastq.gz , ${srr}_2.fastq.gz ) the output files ( cleaned/${srr}_1.fastq.gz , cleaned/${srr}_2.fastq.gz ) Although fastp produces its own reports, we will again use fastqc and multiqc so that we can compare our cleaned and raw data-sets. Perform QC analysis on the cleaned data-sets ls cleaned/*.fastq.gz | xargs fastqc multiqc --title \"Cleaned Reads\" cleaned The resulting multiqc report will be entitled Cleaned-Reads_multiqc_report.html . Double click this file in Jupyter to open the tab and select \"Trust HTML\" to enable the JavaScript figures as described above for the raw data-sets. As none of our data-sets suffers from serious problems, when we compare the Raw and Cleaned reports, there are not many obvious changes. One difference that is easily noticeable is found in the section \" Adapter Content \". Although only a small proportion of reads were affected by lingering adapter sequence in the raw data-sets, fastp has removed what remained. Without too much effort, we have eliminated a source of systematic error -- tools downstream will no longer be tasked with reconciling an artifact of data collection with real observations.","title":"Cleaning up reads with fastp"},{"location":"qc/#taxonomic-analysis","text":"Metagenome taxonomic analysis offers a means to estimate a microbial community profile from metagenomic sequence data. It can give us a very high-level, rough idea of what kinds of microbes are present in a sample. It can also give an idea of how complex/diverse the microbial community is -- whether there are many species or few. It is useful as an initial quality check to ensure that the microbial community composition looks roughly as expected, and to confirm that nothing obvious went wrong during the sample collection and sequencing steps.","title":"Taxonomic analysis"},{"location":"qc/#taxonomic-analysis-with-metaphlan2","text":"While it may be possible to install metaphlan2 via conda, at least in my experience, conda struggles with \"solving the environment\". Therefore it's suggested to install it via the simple download method described on the metaphlan tutorial page : Skip downloading metaphlan2 The >500MB Metaphlan2 package has already been downloaded and we'll just be extracting it. Obtaining metaphlan # this would be how you could download the package yourself # wget -c -O metaphlan.tar.bz2 https://bitbucket.org/nsegata/metaphlan/get/default.tar.bz2 # extract the archive cd ; tar xvjf /data/metaphlan.tar.bz2 # rename the folder to something simple mv nsegata-metaphlan* metaphlan # apply a technical fix for metaphlan plotting sed -i 's/axisbg/facecolor/' metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py # install dependencies conda install -y -n workshop bowtie2 numpy scipy matplotlib Once metaphlan has been prepared we can run it on our QC samples: Running metaphlan on our data cd ~/qc_data for pig in SRR9323808 SRR9323810 SRR9323811 SRR9323809 do zcat ${ pig } *.fastq.gz | ~/metaphlan/metaphlan.py --input_type multifastq --bt2_ps very-sensitive \\ --bowtie2db ~/metaphlan/bowtie2db/mpa --bowtie2out ${ pig } .bt2out -o ${ pig } .mph2 done Unpacking the commands The above series of commands is another exampe of a Bash \"for loop\", where each iteration processes one of our metagenome samples. As before, this is a convenient and less error-prone way to process many samples by avoiding the need to type out the commands for each sample. At each loop iteration, one of the sample names is placed into the loop variable ${pig} , where it can get used in the command inside the loop. Secondly, we decompress ( zcat ) and then feed each pig's sequencing data to metaphlan through a pipe ( | ) Finally we can plot the taxonomic profile of the samples: Plot the taxonomic profile of each sample ~/metaphlan/utils/merge_metaphlan_tables.py *.mph2 > pig_mph2.merged ~/metaphlan/plotting_scripts/metaphlan_hclust_heatmap.py -c bbcry --top 25 --minv 0.1 -s log --in pig_mph2.merged --out mph2_heatmap.png Once that has completed successfully, a new file called mph2_heatmap.png will appear in the qc_data folder of our Jupyter file browser. We can double click it to view. There are other ways to visualize the data, and they are described in the graphlan section of the metaphlan tutorial page.","title":"Taxonomic analysis with Metaphlan2"},{"location":"qc/#taxonomic-analysis-with-other-tools","text":"There are a whole range of other software tools available for metagenome taxonomic analysis. They all have strengths and weaknesses. A few other commonly used tools are listed here: kraken2 MEGAN Centrifuge CLARK","title":"Taxonomic analysis with other tools"},{"location":"qc/#evaluating-the-host-genomic-content","text":"In many applications of metagenomics we are working with host-associated samples. The samples might have come from an animal gut, mouth, or other surface. Similarly for plants we might be working with leaf or root surfaces or rhizobia. When such samples are collected the resulting DNA extracts can include a significant fraction of host material. Let's have a look at what this looks like in data. To do so, we'll download another set of pig gut samples: a set of samples that was taken using scrapings or swabs from different parts of the porcine digestive system, including the duodenum, jejunum, ileum, colon, and caecum. Rather than using metaphlan2 to profile these, we will use the kraken2 classifier in conjunction with the bracken tool for estimating relative abundances. We first need to install kraken2 and bracken : Install kraken2 and braken conda install -y -n workshop kraken2 bracken Next, we need a kraken2 database. For this tutorial we will simply use a precomputed kraken2 database, note however, the very important limitation that the only non-microbial genome it includes is the human genome. If you would like to evaluate host genomic content on plants or other things, you should follow the instructions on the kraken2 web site to build a complete database. Skip downloading the database The Kraken team maintain this and a number of other prepared databases which they make available for download. However, as a matter of courtesy, we will avoid repeatedly requesting the same data. The archive has already been downloaded and we will just be extracting the contents as follows: Extract the precomputed kraken2 reference database cd ; tar xvzf /data/minikraken2_v2_8GB_201904_UPDATE.tgz Finally we are ready to profile our samples with kraken2 and bracken . We'll first download the sample set with parallel-fastq-dump and then run kraken2 and bracken 's est_abundance.py script on each sample using a bash for loop. The analysis can be run as follows: Download some new samples # the list of samples samples = \"SRR9332442 SRR9332438 SRR9332439 SRR9332443 SRR9332440\" # download the associated data parallel-fastq-dump/parallel-fastq-dump ${ samples //SRR/-s SRR } --threads 4 --split-files \\ --gzip --outdir host_qc/ # switch to the data directory and analyse cd host_qc for s in $samples do # run kraken2 analysis kraken2 --paired ${ s } _1.fastq.gz ${ s } _2.fastq.gz --db ~/minikraken2_v2_8GB_201904_UPDATE/ \\ --report ${ s } .kreport > ${ s } .kraken # estimate abundance with braken est_abundance.py -i ${ s } .kreport -o ${ s } .bracken \\ -k ~/minikraken2_v2_8GB_201904_UPDATE/database150mers.kmer_distrib done Once the above has completed, navigate over to the host_qc folder in the Jupyter file browser and click on the *.bracken files to open them.","title":"Evaluating the host genomic content"},{"location":"qc/#what-do-you-see","text":"In particular, why does sample SRR9332438 look so different to sample SRR9332440? Keep in mind the isolation sources of the samples were as follows: Sample Source SRR9332442 Duodenum SRR9332440 Caecum SRR9332439 Ileum SRR9332438 Jejunum SRR9332443 Colon","title":"What do you see?"},{"location":"qc/#some-challenge-questions","text":"If we sequenced samples from pigs, why is human DNA being predicted in these samples? If we were to design a large study around these samples which of them would be suitable for metagenomics, and why? How much sequencing data would we need to generate from sample SRR9332440 to reconstruct the genome of the Bifidobacterium in that sample? What about the E. coli ? Are there really six species of Lactobacillus present in SRR9332440? Go to the NCBI SRA search tool and find a metagenome of interest to you. Download the first 100000 reads from it (use the --maxSpotId parameter) and analyse it with kraken2. Is it what you expected? How does it compare to the others we've looked at?","title":"Some challenge questions"},{"location":"qc/#a-note-on-negative-controls","text":"Negative controls are a key element in any microbiome profiling or metagenome analysis work. Every aspect of the sample collection and processing can be impacted by the presence of contaminating microbial DNA. This is true even of 'sterile' material -- just because no viable cells exist in a sample collection swab, for example, does not mean there is no microbial DNA on that swab. It is well known that molecular biology reagents frequently contain contaminating DNA. Usually it is at low levels, but this is not always the case. Therefore, the best practice is to collect multiple negative control samples that are taken all the way through sequencing. These negative controls can then be used to correct for contamination artifacts in the remaining samples.","title":"A note on negative controls"},{"location":"startup/","text":"Setting up a working environment \u00b6 Logging into Jupyter Hub \u00b6 Log into your Jupyter Hub account with the supplied username and password. Once Jupyter Lab appears, from the Launcher page on the right, open a terminal session by clicking the Terminal icon found in the section Other . Doing so will open a Bash shell (a command-line interface) which we will use to perform our analyses. Info Much of bioinformatical analysis is driven by the command-line and a researcher interested in carrying out their own work will find it essential to develop some profficiency. Though it may seem antiquated to the uninitiated, the command-line is a powerful means of controlling a computer. Installing conda \u00b6 Conda is a user-space package management tool which aims to simplify obtaining and setting up complicated software tools. Although it is not a perfect solution, it allows users to install both a software package and the required dependencies without the help of a system-administrator. Two other approaches to this problem are Docker and Singularity. These tools use the operating system concept of lightweight containers to compartmentalise software into tiny walled gardens. This approach is likely to become the standard means of accessing bioinformatics software in the future. To install conda, copy and paste the commands below to download and then run the installer: Setting up conda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Don't go too fast here Be sure to answer yes to the license agreement and the question about installing the configuration to the .bashrc file. If not, conda will not be configured properly. Close and reopen your shell \u00b6 After the installation has completed, close your terminal session and reopen it. This is the easiest method of properly initialising the shell to use conda. If the installation has been successful, there should be no error messages and your shell prompt will now include the label (base) . This label changes depending on conda environment you have activated and by default you start in \"base\". Lets prepare conda for bioinformatics \u00b6 Conda obtains software that you request from online repositories, which it calls channels. Since this is a general purpose tool, the default repositories contain little in the way of scientific software. To remedy this situation, we will add two channels: conda-forge (a broad general repository) and bioconda (specialist tools related to biological analysis). Once added, conda will search for software in these extra channels as well. Additional conda channels conda config --add channels conda-forge --add channels bioconda And we're ready \u00b6 You should now be ready to start analysing biological data. Let's move on to the next section dealing Quality Control Ways to get started without using Amazon Web Services \u00b6 Not everyone will have access to Amazon EC2, or may not have access all the time. Another alternative is to run a Linux VM locally, provided that you have access to a computer with enough RAM and free storage space. One way to do this is to install VirtualBox on your machine, and then obtain an Ubuntu 18.04 LTS image to start up. This tutorial won't cover how to do this but don't worry, it's not hard. Plenty of people have documented how to do it. Look around, duckduckgo is your friend.","title":"Jupyter Setup"},{"location":"startup/#setting-up-a-working-environment","text":"","title":"Setting up a working environment"},{"location":"startup/#logging-into-jupyter-hub","text":"Log into your Jupyter Hub account with the supplied username and password. Once Jupyter Lab appears, from the Launcher page on the right, open a terminal session by clicking the Terminal icon found in the section Other . Doing so will open a Bash shell (a command-line interface) which we will use to perform our analyses. Info Much of bioinformatical analysis is driven by the command-line and a researcher interested in carrying out their own work will find it essential to develop some profficiency. Though it may seem antiquated to the uninitiated, the command-line is a powerful means of controlling a computer.","title":"Logging into Jupyter Hub"},{"location":"startup/#installing-conda","text":"Conda is a user-space package management tool which aims to simplify obtaining and setting up complicated software tools. Although it is not a perfect solution, it allows users to install both a software package and the required dependencies without the help of a system-administrator. Two other approaches to this problem are Docker and Singularity. These tools use the operating system concept of lightweight containers to compartmentalise software into tiny walled gardens. This approach is likely to become the standard means of accessing bioinformatics software in the future. To install conda, copy and paste the commands below to download and then run the installer: Setting up conda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Don't go too fast here Be sure to answer yes to the license agreement and the question about installing the configuration to the .bashrc file. If not, conda will not be configured properly.","title":"Installing conda"},{"location":"startup/#close-and-reopen-your-shell","text":"After the installation has completed, close your terminal session and reopen it. This is the easiest method of properly initialising the shell to use conda. If the installation has been successful, there should be no error messages and your shell prompt will now include the label (base) . This label changes depending on conda environment you have activated and by default you start in \"base\".","title":"Close and reopen your shell"},{"location":"startup/#lets-prepare-conda-for-bioinformatics","text":"Conda obtains software that you request from online repositories, which it calls channels. Since this is a general purpose tool, the default repositories contain little in the way of scientific software. To remedy this situation, we will add two channels: conda-forge (a broad general repository) and bioconda (specialist tools related to biological analysis). Once added, conda will search for software in these extra channels as well. Additional conda channels conda config --add channels conda-forge --add channels bioconda","title":"Lets prepare conda for bioinformatics"},{"location":"startup/#and-were-ready","text":"You should now be ready to start analysing biological data. Let's move on to the next section dealing Quality Control","title":"And we're ready"},{"location":"startup/#ways-to-get-started-without-using-amazon-web-services","text":"Not everyone will have access to Amazon EC2, or may not have access all the time. Another alternative is to run a Linux VM locally, provided that you have access to a computer with enough RAM and free storage space. One way to do this is to install VirtualBox on your machine, and then obtain an Ubuntu 18.04 LTS image to start up. This tutorial won't cover how to do this but don't worry, it's not hard. Plenty of people have documented how to do it. Look around, duckduckgo is your friend.","title":"Ways to get started without using Amazon Web Services"}]}